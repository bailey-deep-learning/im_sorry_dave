{
  
    
        "post0": {
            "title": "Please put down your weapon.",
            "content": "Image Classification . Up to this point we have created a Neural Network using only 4 lines of code with fastai&#39;s high level API and we have created a Neural Network from scratch using PyTorch only for its tensors and back-propogation functions. Now we are going to start trying to coalesce these two methodologies and attempt to meet somewhere in the middle. . In this chapter we are looking at multi-image classification, it is quite a step up from the work we did in Chapter 2 where we identified Star Wars Robot or the simple binary classification in Chapter 4 where we attempted to disambiguate between hand written characters of 3s and 7s. Here we are going to attempt to identify breeds of cats and dogs with 37 different catagories. It is no mean feat. . Also if you are interested in the name of this blog post, its one of my favourite scenes from Robocop, where ED-209 cannot identify whether or not Kinney has reliquished his firearm. Let&#39;s hope we can do a better job here! . From Dogs and Cats to Pet Breeds . Thankfully the fantastic team at fastai have provided a dataset for us, moving forward and looking at our own problems we will need to create them ourselves, but for now we can simply run untar_data and pass in the URLs.PETS url to download, extract and return a local path to the data . from fastai.vision.all import * path = untar_data(URLs.PETS) . Again here we can use the ls function to look inside our directory to what our dataset folder contains . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . Lets take a look inside the images folder. Here we can see that we have 7393 images and the first ten paths. Similarly to our example in Chapter 1, file names that start with a capital letter are Cat breeds and file names beginning with lowercase letters are Dog breeds. But we need to differtiate them even further in this example, into the specific breeds. . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/Ragdoll_197.jpg&#39;),Path(&#39;images/newfoundland_36.jpg&#39;),Path(&#39;images/boxer_107.jpg&#39;),Path(&#39;images/scottish_terrier_5.jpg&#39;),Path(&#39;images/yorkshire_terrier_136.jpg&#39;),Path(&#39;images/keeshond_21.jpg&#39;),Path(&#39;images/beagle_188.jpg&#39;),Path(&#39;images/chihuahua_139.jpg&#39;),Path(&#39;images/basset_hound_44.jpg&#39;),Path(&#39;images/Russian_Blue_135.jpg&#39;)...] . We can do that using a Regular Expression. The format of the file names are: . {ANIMALBREED} {INTEGER}.{EXTENSION} . Regular expressions are a fantastic utility that can parse strings are return specific data from them. There is a great resource that can be found as part of the fastai nlp course. First lets store first image path into a variable. . fname = (path/&quot;images&quot;).ls()[0] . Here by using our regular expression r&#39;(.+)_ d+.jpg$&#39; we are able to extract the name of our first breed. . (.+) this part of the expression is looking for the breed using re.findall() and it will return that value. This represents the {ANIMAL_BREED} part of the name format shown above. . _ d+.jpg$&#39; this part of the expression is looking for the underscore _ the {INTEGER} d+ and the .{EXTENSION} .jpg . Here we can see it worked! By passing in our first image path our Regular Expression returns Ragdoll . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;Ragdoll&#39;] . So now, lets create a DataBlock from our dataset. We define that our Dependent Variable is of type ImageBlock and our Independent Variable is of type CatagoryBlock. . Here we are using a new function and a new class. Here we are passing RegexLabeller object and the name variable to fastai&#39;s using_attr function. This will perform our regex segmentation on every files name attribute to return our image labels into the DataBlocks get_y argument. . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . We also have two new lines that perform resizing operations. Once on a per item level and one at a batch level, this is a process the team at fastai call Presizing . Presizing . . Resize images to relatively &quot;large&quot; dimensions—that is, dimensions significantly larger than the target training dimensions. | Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times. | . The first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller. . In the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end. . dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) dls1 = dblock1.dataloaders([(Path.cwd().parent/&#39;images&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . As you can see this Presizing process gives significantly better quality results . Checking and Debugging a DataBlock . So now we can check out our DataBlock by using show_batch . dls.show_batch(nrows=1, ncols=3) . This example shows us a summary of a DataBlock if we didn&#39;t utilise our item_tfms and batch_tfms. . Sets up transforms pipeline | Builds one sample | Collects the items | Builds one batch | . Before it falls over because our images are of different dimensions. summary is really useful to see what has happened in building your DataBlock and image tranformation pipeline and gives detailed explaination of the the errors . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /storage/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} . Building one sample Pipeline: PILBase.create starting from /storage/data/oxford-iiit-pet/images/english_setter_53.jpg applying PILBase.create gives PILImage mode=RGB size=500x400 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /storage/data/oxford-iiit-pet/images/english_setter_53.jpg applying partial gives english_setter applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(19) . Final sample: (PILImage mode=RGB size=500x400, TensorCategory(19)) . Collecting items from /storage/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} . Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=500x400, TensorCategory(19)) applying ToTensor gives (TensorImage of size 3x400x500, TensorCategory(19)) . Adding the next 3 samples . No before_batch transform to apply . Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes (3, 400, 500),(3, 225, 300),(3, 225, 300),(3, 332, 500) . Now lets create a cnn_learner pass in our dataloader along with the resnet34 architecture and train the last 2 layers of the network. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.493833 | 0.319699 | 0.108254 | 01:41 | . epoch train_loss valid_loss error_rate time . 0 | 0.520890 | 0.307597 | 0.102165 | 02:12 | . 1 | 0.341706 | 0.230789 | 0.075101 | 02:11 | . But we didn&#39;t specify a loss function, thankfully fastai knows which loss fucntions are useful for each type of learner. So what did it pick? . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . So what is Cross-Entropy Loss? . Cross-Entropy Loss . Viewing Activations and Labels . A sensible place to start is taking a look at our data, here we can destructure one_batch of from our data loader and see our independent variable y . x,y = dls.one_batch() . y . TensorCategory([34, 23, 4, 25, 32, 4, 33, 28, 16, 17, 0, 21, 5, 9, 20, 8, 31, 11, 34, 21, 13, 10, 20, 4, 35, 17, 25, 31, 32, 3, 9, 35, 32, 30, 5, 31, 6, 15, 31, 23, 17, 33, 7, 20, 33, 18, 31, 18, 21, 24, 20, 33, 10, 1, 32, 21, 1, 17, 23, 23, 19, 4, 11, 21], device=&#39;cuda:0&#39;) . Here we can see all of our classifications, each index value represents one of the breeds within our dataset. We can see what that index value pertains to by using the vocab function, but it isn&#39;t entirely relevant in this example. . Let&#39;s now make a prediction using get_preds . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.4188e-06, 9.2919e-07, 8.1356e-05, 4.2431e-06, 1.2433e-05, 6.9899e-06, 2.5417e-06, 8.4320e-06, 4.2290e-06, 1.2586e-04, 1.9387e-04, 3.8595e-05, 2.4883e-02, 6.4488e-03, 2.0814e-06, 1.1752e-04, 1.0921e-04, 2.7717e-02, 2.0989e-06, 3.3488e-05, 2.0668e-05, 2.7205e-03, 2.6016e-05, 5.9952e-05, 5.7038e-05, 2.0636e-06, 5.0264e-05, 9.4108e-06, 6.0596e-06, 1.1552e-03, 3.6774e-04, 1.9417e-04, 5.5275e-05, 1.2116e-04, 9.3534e-01, 7.1628e-06, 1.4299e-05]) . Here we can see that the highest value prediction is 9.3534e-01 and it is significantly higher than any of the other predictions. There is also another attribute of these predictions . len(preds[0]),preds[0].sum() . (37, tensor(1.)) . They all sum to 1. . Softmax . Lets start with a simple example from Chapter 4, lets draw a graph of the sigmoid function . plot_function(torch.sigmoid, min=-4,max=4) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . If we remember correctly by passing in extremely high numbers the result with tend towards 1, and extremely low numbers will tend towards 0. This will aid us in normalizing our predicted results. . If we create some simple example data that could be representative of the predictions of whether an input to the network is either a 3 or a 7 we might get a tensor similar to our data below . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . If we pass these values through our sigmoid function we get two new values, but they aren&#39;t particularly helpful in giving us a concrete predicition on whether or not our inputs are 3s or 7s, so how can we remedy this? . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . Maybe instead of looking at the values independently we want to pass the difference of the predictions to the sigmoid fucntion and have 1 - the sigmoid of the difference being the resultant prediction . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . The additional benefit is that the results are guaranteed to sum to 1 effectively giving us a percentage. . 60.25% chance of being a 3 vs 39.75% chance of being a 7 | . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Note: The softmax function looks like this: . def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True) . Exponential function (exp): Literally defined as e**x, where e is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that exp is always positive, and it increases very rapidly! . If we have three output activations, such as in our droid classifier, calculating softmax for a single droid image would then look like something like this: . . Log Likelihood . So now that we have our predictions how do calculate our loss? . If we remeber back in Chapter 4, we created our mnist_loss function . def mnist_loss(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).mean() . Here we determined that if the target was 1 we would return 1 - our input else the input value, this would tell us how far our prediction was from the actual target. . But this will only work in the binary case, what if we would like to solve for more classes? We can simply index into our softmax activations! Sounds to good to be true. Lets take a look at our toy example above . targ = tensor([0,1,0,1,1,0]) . Here we have defined our target indices in a tensor . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Our softmax activations are the same as they are above . But by looking into each row and retrieving only the column indexed in that row we can determine our loss . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . The nice thing about this method is that it scales to any number of catagories. If we were doing the full MNIST classification, identifying all ten digits, we would be indexing into a column of ten values instead of the two in this example . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . By making our softmax activations negative, we are getting the identical results as the PyTorch function nll_loss or Negative Log Likelihood, here we can see that the function provided by PyTorch and fastai are simple shortcuts to work we can do ourselves . warning: Confusing Name, Beware: The nll in nll_loss stands for &quot;negative log likelihood,&quot; but it doesn&#39;t actually take the log at all! It assumes you have already taken the log. PyTorch has a function called log_softmax that combines log and softmax in a fast and accurate way. nll_loss is designed to be used after log_softmax. . Taking the Log . y = b**a . a = log(y,b) . In this case, we&#39;re assuming that log(y,b) returns log y base b. However, PyTorch actually doesn&#39;t define log this way: log in Python uses the special number e (2.718...) as the base. . Perhaps a logarithm is something that you have not thought about for the last 20 years or so. But it&#39;s a mathematical idea that is going to be really critical for many things in deep learning, so now would be a great time to refresh your memory. The key thing to know about logarithms is this relationship: . log(a*b) = log(a)+log(b) . plot_function(torch.log, min=0,max=4) . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . F.cross_entropy(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . An interesting feature about cross-entropy loss appears when we consider its gradient. The gradient of cross_entropy(a,b) is just softmax(a)-b. Since softmax(a) is just the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming there&#39;s no final activation function such as that added by y_range), since the gradient of (a-b)*2 is 2(a-b). Because the gradient is linear, that means we won&#39;t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models. . Model Interpretation . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;Ragdoll&#39;, &#39;Birman&#39;, 10), (&#39;miniature_pinscher&#39;, &#39;chihuahua&#39;, 6)] . Improving Our Model . The Learning Rate Finder . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.654461 | 4.516049 | 0.505413 | 01:40 | . epoch train_loss valid_loss error_rate time . 0 | 4.329690 | 8.718549 | 0.822733 | 02:12 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 4.37e-03 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.352067 | 0.302961 | 0.094723 | 01:40 | . epoch train_loss valid_loss error_rate time . 0 | 0.533749 | 0.302427 | 0.086604 | 02:11 | . 1 | 0.334618 | 0.228772 | 0.072395 | 02:11 | . Unfreezing and Transfer Learning . learn.fine_tune?? . Signature: learn.fine_tune( epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, ) Source: @patch @delegates(Learner.fit_one_cycle) def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, kwargs): &quot;Fine tune with freeze for freeze_epochs then with unfreeze from epochs using discriminative LR&quot; self.freeze() self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, kwargs) base_lr /= 2 self.unfreeze() self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs) File: /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py Type: method . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.102643 | 0.279781 | 0.094723 | 01:40 | . 1 | 0.535196 | 0.246822 | 0.076455 | 01:40 | . 2 | 0.327852 | 0.198139 | 0.066306 | 01:40 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=1.9054607491852948e-07, lr_steep=1.5848931980144698e-06) . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.255147 | 0.191486 | 0.060217 | 02:11 | . 1 | 0.255074 | 0.178717 | 0.060217 | 02:11 | . 2 | 0.228266 | 0.178046 | 0.064276 | 02:12 | . 3 | 0.209520 | 0.178112 | 0.056157 | 02:11 | . 4 | 0.186808 | 0.177464 | 0.056157 | 02:11 | . 5 | 0.184688 | 0.172256 | 0.054804 | 02:12 | . Discriminative Learning Rates . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.139847 | 0.307070 | 0.099459 | 01:41 | . 1 | 0.536971 | 0.257561 | 0.081867 | 01:40 | . 2 | 0.338579 | 0.213168 | 0.071042 | 01:40 | . epoch train_loss valid_loss error_rate time . 0 | 0.267892 | 0.210070 | 0.069012 | 02:12 | . 1 | 0.247653 | 0.196744 | 0.066982 | 02:12 | . 2 | 0.242534 | 0.189713 | 0.062923 | 02:12 | . 3 | 0.213890 | 0.183064 | 0.059540 | 02:12 | . 4 | 0.201939 | 0.182476 | 0.066306 | 02:12 | . 5 | 0.173591 | 0.167041 | 0.052097 | 02:13 | . 6 | 0.165675 | 0.169903 | 0.055480 | 02:13 | . 7 | 0.151925 | 0.167067 | 0.053451 | 02:12 | . 8 | 0.135530 | 0.170335 | 0.052774 | 02:12 | . 9 | 0.122693 | 0.162749 | 0.052097 | 02:12 | . 10 | 0.121328 | 0.163464 | 0.049391 | 02:12 | . 11 | 0.126313 | 0.166235 | 0.052097 | 02:13 | . learn.recorder.plot_loss() . Selecting the Number of Epochs . Deeper Architectures . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.421636 | 0.300777 | 0.099459 | 02:23 | . 1 | 0.621208 | 0.274879 | 0.085250 | 02:24 | . 2 | 0.435900 | 0.276002 | 0.087957 | 02:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.278140 | 0.283758 | 0.089986 | 03:13 | . 1 | 0.315363 | 0.287254 | 0.081867 | 03:14 | . 2 | 0.275569 | 0.261106 | 0.074425 | 03:17 | . 3 | 0.157947 | 0.233280 | 0.059540 | 03:15 | . 4 | 0.084224 | 0.203565 | 0.059540 | 03:15 | . 5 | 0.061835 | 0.193485 | 0.054127 | 03:15 | . Conclusion . Questionnaire . Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? . This concept is known as presizing. Data augmentation is often applied to the images and in fastai it is done on the GPU. However, data augmentation can lead to degradation and artifacts, especially at the edges. Therefore, to minimize data destruction, the augmentations are done on a larger image, and then RandomResizeCrop is performed to resize to the final image size. . If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book&#39;s website for suggestions. . See above . What are the two ways in which data is most commonly provided, for most deep learning datasets? . Individual files representing items of data, such as text documents or images. | A table of data, such as in CSV format, where each row is an item, each row which may include filenames providing a connection between the data in the table and data in other formats such as text documents and images. | . Give two examples of ways that image transformations can degrade the quality of the data. . Rotation can leave empty areas in the final image | Other operations may require interpolation which is based on the original image pixels but are still of lower image quality | . What method does fastai provide to view the data in a DataLoaders? . DataLoader.show_batch . What method does fastai provide to help you debug a DataBlock? . DataBlock.summary . Should you hold off on training a model until you have thoroughly cleaned your data? . No. It is best to create a baseline model as soon as possible. . What are the two pieces that are combined into cross-entropy loss in PyTorch? . Cross Entropy Loss is a combination of a Softmax function and Negative Log Likelihood Loss. . What are the two properties of activations that softmax ensures? Why is this important? . It makes the outputs for the classes add up to one. This means the model can only predict one class. Additionally, it amplifies small changes in the output activations, which is helpful as it means the model will select a label with higher confidence (good for problems with definite labels). . When might you want your activations to not have these two properties? . When you have multi-label classification problems (more than one label possible). . Why can&#39;t we use torch.where to create a loss function for datasets where our label can have more than two categories? . Because torch.where can only select between two possibilities while for multi-class classification, we have multiple possibilities. . What is the value of log(-2)? Why? . This value is not defined. The logarithm is the inverse of the exponential function, and the exponential function is always positive no matter what value is passed. So the logarithm is not defined for negative values. . What are two good rules of thumb for picking a learning rate from the learning rate finder? . Either one of these two points should be selected for the learning rate: . one order of magnitude less than where the minimum loss was achieved (i.e. the minimum divided by 10) | the last point where the loss was clearly decreasing. | . What two steps does the fine_tune method do? . Train the new head (with random weights) for one epoch | Unfreeze all the layers and train them all for the requested number of epochs | . In Jupyter Notebook, how do you get the source code for a method or function? . Use ?? after the function ex: DataBlock.summary?? . What are discriminative learning rates? . Discriminative learning rates refers to the training trick of using different learning rates for different layers of the model. This is commonly used in transfer learning. The idea is that when you train a pretrained model, you don’t want to drastically change the earlier layers as it contains information regarding simple features like edges and shapes. But later layers may be changed a little more as it may contain information regarding facial feature or other object features that may not be relevant to your task. Therefore, the earlier layers have a lower learning rate and the later layers have higher learning rates. . How is a Python slice object interpreted when passed as a learning rate to fastai? . The first value of the slice object is the learning rate for the earliest layer, while the second value is the learning rate for the last layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. . Why is early stopping a poor choice when using 1cycle training? . If early stopping is used, the training may not have time to reach lower learning rate values in the learning rate schedule, which could easily continue to improve the model. Therefore, it is recommended to retrain the model from scratch and select the number of epochs based on where the previous best results were found. . What is the difference between resnet50 and resnet101? . The number 50 and 101 refer to the number of layers in the models. Therefore, ResNet101 is a larger model with more layers versus ResNet50. These model variants are commonly as there are ImageNet-pretrained weights available. . What does to_fp16 do? . This enables mixed-precision training, in which less precise numbers are used in order to speed up training. . Further Research . Find the paper by Leslie Smith that introduced the learning rate finder, and read it. | See if you can improve the accuracy of the classifier in this chapter. What&#39;s the best accuracy you can achieve? Look on the forums and the book&#39;s website to see what other students have achieved with this dataset, and how they did it. |",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/17/Put-Down-Your-Weapon.html",
            "relUrl": "/jupyter/2020/12/17/Put-Down-Your-Weapon.html",
            "date": " • Dec 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "All those moments will be los(s)t in time, like tears in rain.",
            "content": "Under the Hood: Training a Digit Classifier . This one is the big one. Now we have made it past the introduction of the course it is time to get under the hood and start implementing some of the functionality for ourselves. I am going to be taking quite thorough notes as I want to make sure I understand everything before I move onto Chapter 5. . So far all the heavy lifting has been done for us, the fastai library is a nice high level API written on top of PyTorch and abstracted in such a way that the &quot;magic&quot; / &quot;obfuscation&quot; is a little hard to determine what is actually happening. So in this chapter we will be making a hand written digit classifier, a simple classifier that can determine whether or not a 28px * 28px image of a hand written digit is either a 3 or a 7. We will try to figure out a good baseline from which we can assess our model and then proceed to write out each element of the model in simple python, before seeing it all wrapped up in the nice and tidy fastai API. . Pixels: The Foundations of Computer Vision . Here we are going to take a look at what we are actually dealing with when it comes to our hand written digits. Thankfully there is a great training set put together by Yann LeCun called the MNIST or Modified National Institute of Standards and Technology database. It contains thousands of individual hand written digits that have been collated as 28px * 28px grayscale images. This is the data we will be using to build our classifier. . The team at fastai have made it simple to download and unzip the data we are going to use for this lesson. Instead of having to manually go to the fastai datasets documentation page, download the tar file, unzip it in a location accessible to your notebooks they have a handy utility that will do all of that in single line of code: . path = untar_data(URLs.MNIST_SAMPLE) . The have also added the UNIX ls() function to Python path to give a handy way to see what is in our directory. Here you can see we have our training and validation directory along with a label.csv file. . path.ls() . (#3) [Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;train&#39;)] . Inside each of the training and validation directories we have a folder which contains all of our 3&#39;s and 7&#39;s respectively . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . Now we can create a list of the file paths and store them in variables . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . Let&#39;s take a look at what we are working with. By indexing into the above threes list we can retrive the first path in the list. Using PIL the Python Imaging Library, we can display the data at that path. . im3_path = threes[1] im3 = Image.open(im3_path) im3 . At this point it may not be entirely intuative what image information is. It is just an array of values for 0 to 255 for an 8-bit grayscale image like we have above. By casting into a Numpy array and taking a slice we can se what that might look like. . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . As we have seen earlier in the course the PyTorch tensors have very similar functionality to Numpy arrays, but have the added benefit of being pushed to the GPU, this is a optimization and can be used in replacement of standard Numpy arrays. As a huge fan of Numpy my natural propensity is to use them all the time. But I think I need to reconsider . . . . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . By loading the tensor into a Pandas Dataframe we are able to see what the pixel values look like by using the .background_gradients method . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . . First Try: Pixel Similarity . Before we start building our model we should consider if there are feasible alternatives to our problem. Working with Deep Learning is like weilding a very large hammer and every problem can appear to us as a nail. But for small self contained problem that do not require scale, simple alternatives are preferable. Here we are exploring an alternative to determine a baseline. What results can we achieve with a naive approach, is this sufficient to solve our problem and can they be improved by the use of a deep learning model. This is an important step, making sure that you are making headway on your problem is more important than having a shiny new model. . Problem Statement . Can we create a simple baseline application that can classify an unseen 28px * 28px image of a 3 or a 7. Theoretically, this is a simpler task than differentiating between a 3 and an 8 for example because both have similar curves at the top of the number and the bottom which might be difficult to disambiguate. If we try making a mean image from all the data in the threes folder and another for the sevens and compare the pixel similarity of an unseen image with this mean image we might be able to classify it with some manner of accuracy . Process . Let&#39;s create two more arrays that store a tensor of each image in our threes and sevens folders. . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . In a similar way to the method above to cast our image to an array or tensor, we can use the fastai show_image method to take a tensor and display it as an image, this will be useful for debugging . show_image(three_tensors[1]); . Our next operation is to normalize the values of the images between 0 and 1, first we stack the images on top of one another using torch.stack, convert the integer values in the tensor stack to a float to ensure that values aren&#39;t rounded after our division operation, and then we divide the image by 255. . By looking at the shape of the torch tensor stack we can see that we have 6131 image tensors that have 28 rows and 28 columns . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . The length of the tensor.shape is the same as the tensor rank or number of dimensions, we can see that below by using the ndim method. This is a fast way of checking that your tensors have the correct rank before moving forward building your model or baseline . len(stacked_threes.shape) . 3 . stacked_threes.ndim . 3 . Now that we have all of our image tensors stack on top of one another and normalized, we can use the mean method to find the average. By passing in the argument 0, we are telling the operation to take the mean across the first axis. In this example this is the mean of the 6131 images. . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . Now that we have our mean images, we can compare one of the images against them to see what the pixel similarity is and determine if the image is either a 3 or a 7 . a_3 = stacked_threes[1] show_image(a_3); . L1 and L2 norm . Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm | Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. | . To determine the pixel similarity of our test image and our mean image we are going to use the mean absolte difference or the L1 Norm and the root mean squared error or the L2 Norm. . If we were simply going to subtract one for the other we could end up with negative values, which once averaged would negate positive values and give us inconcluse information. By comparing the test image againt the mean of the 3&#39;s and the mean of the 7&#39;s we can see that error is lower when comparing to the mean of the 3&#39;s giving us a classification that this image is of a 3 . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . Here we can see by using the l1_loss and the mse_loss methods we are getting the same answers our implementations above . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . Computing Metrics Using Broadcasting . At this point we could simply use a loop and iterate through each of the images in the validation set, calculate the L1 loss for each image and determine whether or not our baseline application determines the number to be a 3 or a 7, check the prediction against and determine an accuracy. This would take a very long time and wouldn&#39;t make use of the GPU accelleration needed for Deep Learning. . Here we are going to look at the secret sauce that makes Python (an interpreted language) powerful enough to be used in Deep Learning applications. What is that secret sauce you ask? Broadcasting . So lets start by stacking the images from our validation directories in the same way we did with our training images. We will normalize them and check the shape of the tensor stack . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Here we are going to write our L1 Norm in the form of a function, here taking the mean across the rows and columns of the absolute difference of the tow images. . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . But by using Broadcasting, we can instead use our entire tensor stack and compare it against the mean image of the 3 all at once. What is happening under the hood is that PyTorch is making a &quot;virtual&quot; copy of the mean image tensor for every image tensor in the validation stack so it can determine the pixel similarity for all of them at once. What it returns is a tensor of all of the results. . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1605, 0.1165, 0.1405, ..., 0.1159, 0.1183, 0.1471]), torch.Size([1010])) . Now lets create a simple definition to determine if the prediction is a 3 if the result is False then we assume that the image is classified as a 7 . If the mnist_distance measured againt the mean 3 is lower than the mean 7 then the function will return True . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . Here by passing in our test image we can see that it returns True, but by casting it to a float we can get a value instead . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . Taking advantage of Broadcasting we can pass the entire tensor stack to the function and we get back an array of predictions . is_3(valid_3_tens) . tensor([False, True, True, ..., True, True, True]) . Lets check the accuracy of our classification application. We are expecting every image tensor in the valid_3_tens tensor to return true and all the image tensors in the valid_7_tens tensor to return false. We can convert them to a floating point value and take the mean to determine the accuracy . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Wow! It looks like our naive pixel similarity application gives us a 95% accuracy on this task! . We have only used PyTorch for its power tensor operations in this task and proven that with a simple baseline we can determine a highly accurate classifier. Let&#39;s see if making a Deep Learning model we can improve the accuracy even further! . Stochastic Gradient Descent (SGD) . Up until this point we have a classifier but it really does follow the description by Arthur Samuel . Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would &quot;learn&quot; from its experience. . To turn our function into a machine learning classifier we will need: . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | . gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop . To understand SGD a little better lets start with a simpler example using this quadratic function: . def f(x): return x**2 . Let&#39;s plot what that function looks like: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . The sequence of steps we described earlier starts by picking some random value for a parameter, and calculating the value of the loss: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . Now we look to see what would happen if we increased or decreased our parameter by a little bit—the adjustment. This is simply the slope at a particular point: . . We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve: . . This basic idea goes all the way back to Isaac Newton, who pointed out that we can optimize arbitrary functions in this way . Calculating Gradients . This is a lot of text from this portion of the book, its really important and I getting it right is paramount . &quot;The one magic step is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better.&quot; . Thankfully PyTorch is a very powerful auto-differential library. It utilises the Chain Rule to calculate the derivative of our functions. . First, let&#39;s pick a tensor value which we want gradients at: . xt = tensor(3.).requires_grad_() . Notice the special method requiresgrad? That&#39;s the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for. . This API might throw you off if you&#39;re coming from math or physics. In those contexts the &quot;gradient&quot; of a function is just another function (i.e., its derivative), so you might expect gradient-related APIs to give you a new function. But in deep learning, &quot;gradients&quot; usually means the value of a function&#39;s derivative at a particular argument value. The PyTorch API also puts the focus on the argument, not the function you&#39;re actually computing the gradients of. It may feel backwards at first, but it&#39;s just a different perspective. . Now we calculate our function with that value. Notice how PyTorch prints not just the value calculated, but also a note that it has a gradient function it&#39;ll be using to calculate our gradients when needed: . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Calculating the derivative value of our function at this input tensor is simple, we just call the backward method. . yt.backward() . The &quot;backward&quot; here refers to backpropagation, which is the name given to the process of calculating the derivative of each layer. . We can now view the gradients by checking the grad attribute of our tensor: . xt.grad . tensor(6.) . If you remember your high school calculus rules, the derivative of x**2 is 2x, and we have x=3, so the gradients should be 2*3=6, which is what PyTorch calculated for us! . Now we&#39;ll repeat the preceding steps, but with a vector argument for our function: . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . And we&#39;ll add sum to our function so it can take a vector (i.e., a rank-1 tensor), and return a scalar (i.e., a rank-0 tensor): . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . Our gradients are 2*xt, as we&#39;d expect! . yt.backward() xt.grad . tensor([ 6., 8., 20.]) . The gradients only tell us the slope of our function, they don&#39;t actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value. . Stepping With a Learning Rate . Because our gradient only shows the slope, or the direction, in which we need to change our parameters. We will need to figure out how much we move in this direction. This is where we introduce the learning rate. The learning rate is often a number between 0.001 and 0.1, although it could be anything. . Once you&#39;ve picked a learning rate, you can adjust your parameters using this simple function: . w -= gradient(w) * lr . If you pick a learning rate that&#39;s too low, it can mean having to do a lot of steps . . But picking a learning rate that&#39;s too high is even worse—it can actually result in the loss getting worse . . If the learning rate is too high, it may also &quot;bounce&quot; around, rather than actually diverging . . An End-to-End SGD Example . Let&#39;s work through a practical example from end-to-end. First we will make a float tensor that represents our time variable. Each index is a time in seconds . time = torch.arange(0,20).float(); time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . Here we create a quadratic function, a function of the form a(time**2)+(b*time)+c, and add some noiose to simulate realworld measurments . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . We want to distinguish clearly between the function&#39;s input (the time when we are measuring the coaster&#39;s speed) and its parameters (the values that define which quadratic we&#39;re trying). So, let&#39;s collect the parameters in one argument and thus separate the input, t, and the parameters, params, in the function&#39;s signature: . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . We need to determine which loss function we would like to use. For continuous data, it&#39;s common to use mean squared error: . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() . Step 1: Initialize the parameters . First thing we want to do is create a tensor for our parameters and to tell PyTorch that this tensor requires_grad_(). . params = torch.randn(3).requires_grad_() . Step 2: Calculate the predictions . Lets pass our time tensor and our params into our function . preds = f(time, params) . Here we are creating a small matplotlib function to show a comparison between our predictions and our observations . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Step 3: Calculate the loss . Now we can use our L2 or RMSE function to determine our loss . loss = mse(preds, speed) loss . Step 4: Calculate the gradients . By performing Back Propogation with our backward() method on the loss, we can calculate the slope of our gradient and which direction the we need to move in to improve our loss . loss.backward() params.grad . tensor([-165.5151, -10.6402, -0.7900]) . Here we use a small learning rate and multiply that to our parameter&#39;s gradient to make sure we move towards the optimal solution over each iteration . params.grad * 1e-5 . tensor([-1.6552e-03, -1.0640e-04, -7.8996e-06]) . params . tensor([-0.7658, -0.7506, 1.3525], requires_grad=True) . Step 5: Step the weights. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Understanding this bit depends on remembering recent history. To calculate the gradients we call backward on the loss. But this loss was itself calculated by mse, which in turn took preds as an input, which was calculated using f taking as an input params, which was the object on which we originally called requiredgrads—which is the original call that now allows us to call backward on loss. This chain of function calls represents the mathematical composition of functions, which enables PyTorch to use calculus&#39;s chain rule under the hood to calculate these gradients. . preds = f(time,params) mse(preds, speed) . tensor(160.4228, grad_fn=&lt;SqrtBackward&gt;) . show_preds(preds) . Here we simply wrap our previous steps into a function . Make a prediction | Calculate the loss | Perform back propogation on the loss, see above for details | Apply the learning rate | Zero our gradients | Return our predictions | . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: Repeat the process . Now we simply call our apply_step function a number of times, as we can see our loss improves each iteration . for i in range(10): apply_step(params) . 155.75035095214844 155.4757537841797 155.20118713378906 154.92662048339844 154.65211486816406 154.37762451171875 154.1031494140625 153.82872009277344 153.55430603027344 153.27992248535156 . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7: stop . Either we can stop after a certain accuracy or simply after a number of iterations . Summarizing Gradient Descent . gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | . To summarize, at the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning). In the first case, the output we will get from our inputs won&#39;t have anything to do with what we want, and even in the second case, it&#39;s very likely the pretrained model won&#39;t be very good at the specific task we are targeting. So the model will need to learn better weights. . We begin by comparing the outputs the model gives us with our targets (we have labeled data, so we know what result the model should give) using a loss function, which returns a number that we want to make as low as possible by improving our weights. To do this, we take a few data items (such as images) from the training set and feed them to our model. We compare the corresponding targets using our loss function, and the score we get tells us how wrong our predictions were. We then change the weights a little bit to make it slightly better. . To find how to change the weights to make the loss a bit better, we use calculus to calculate the gradients. (Actually, we let PyTorch do it for us!) Let&#39;s consider an analogy. Imagine you are lost in the mountains with your car parked at the lowest point. To find your way back to it, you might wander in a random direction, but that probably wouldn&#39;t help much. Since you know your vehicle is at the lowest point, you would be better off going downhill. By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination. We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take; specifically, we multiply the gradient by a number we choose called the learning rate to decide on the step size. We then iterate until we have reached the lowest point, which will be our parking lot, then we can stop. . The MNIST Loss Function . Now we have seen how the mothod works on a simple function, we can now put this into practice on our MNIST 3&#39;s and 7&#39;s problem. As we have our dependent variable x: . Remember: . Dependent Variable == input variables | Independent Varibale == output variables | . Our dependent variable in this is example are the images themselves. Here we concatinate the stacked image tensors of the 3&#39;s and the 7&#39;s. Having the image as Matrix is irrelevant we can use the Pytorch view method to reshape every tensor to rank 1, . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . We also need a label for each of the images, here we can simply create a tensor by combining an array of 1&#39;s of length of the number of 3&#39;s and an array of 0&#39;s the length of the number of 7&#39;s. We use the PyTorch function unsqueeze to transpose the tensor from a vector with 123396 elements into a tensor with 12396 rows and a single column . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . Now we need to create a dataset, a dataset needs to be able to be indexable, and at that index we expect a tuple (data, label). Here we are using the python zip function to take the train_x and train_y varibles and combine them into a tuple at each index as described . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . We need to do the same operations above for our validation set as well . Concatinate the images and reshape | Create labels and unsqueeze | Zip data and label into dataset | . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . Here is a simple function that will initialize our parameters with random values. PyTorch randn returns a tensor the shape and size of its argument with normalized values between 0 and 1. We can use a variance argument here to scale the random values if necessary, but not in this example. We want to be able to calculate the gradient of this tensor, so we use the requires_grad method . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . We create and initialize our weights and bias variables . weights = init_params((28*28,1)) . bias = init_params(1) . In neural networks, the w in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters. . We can now calculate a prediction for one image: . (train_x[0]*weights.T).sum() + bias . tensor([20.2336], grad_fn=&lt;AddBackward0&gt;) . While we could use a Python for loop to calculate the prediction for each image, that would be very slow. Because Python loops don&#39;t run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions. . In this case, there&#39;s an extremely convenient mathematical operation that calculates w*x for every row of a matrix—it&#39;s called matrix multiplication. . . In Python, matrix multiplication is represented with the @ operator. Let&#39;s try it: . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[20.2336], [17.0644], [15.2384], ..., [18.3804], [23.8567], [28.6816]], grad_fn=&lt;AddBackward0&gt;) . Lets check how good our random initialization is. Here is make a determination that if a prediction is over 0 it&#39;s a 3 else its a 7 and we compare it against our labels . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[ True], [ True], [ True], ..., [False], [False], [False]]) . As you can see, and could have predicted, a random initilization is correct roughly 50% of the time. . corrects.float().mean().item() . 0.4912068545818329 . So lets change one of our parameters a little bit and see if that changes our result . weights[0] *= 1.0001 . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.4912068545818329 . As we can see above changing one parameter a little has absolutely no affect on the results of our network. What does this mean practically? . By changing this pixel by some small amount is not sufficient in itself to change the prediction of an image from a 3 to a 7 . Becaue there is no change we cannot calculate a gradient and we cannot make a step to improve our predictions. This is because of the thresholding in our determining our correctness. . In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model. . So how do we fix this? . trgts = tensor([1,0,1]) prds = tensor([0.9, 0.4, 0.2]) . Here&#39;s a first try at a loss function that measures the distance between predictions and targets: . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() . Read the Docs: It&#39;s important to learn about PyTorch functions like this, because looping over tensors in Python performs at Python speed, not C/CUDA speed! Try running help(torch.where) now to read the docs for this function, or, better still, look it up on the PyTorch documentation site. . torch.where(trgts==1, 1-prds, prds) . tensor([0.1000, 0.4000, 0.8000]) . You can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident. In PyTorch, we always assume that a lower value of a loss function is better. Since we need a scalar for the final loss, mnist_loss takes the mean of the previous tensor: . mnist_loss(prds,trgts) . tensor(0.4333) . For instance, if we change our prediction for the one &quot;false&quot; target from 0.2 to 0.8 the loss will go down, indicating that this is a better prediction: . mnist_loss(tensor([0.9, 0.4, 0.8]),trgts) . tensor(0.2333) . One problem with mnist_loss as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that—let&#39;s take a look. . Sigmoid . The sigmoid function always outputs a number between 0 and 1. It&#39;s defined as follows: . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . As you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. It&#39;s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. . Let&#39;s update mnist_loss to first apply sigmoid to the inputs: . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . SGD and Mini-Batches . In the context of SGD, &quot;Minibatch&quot; means that the gradient is calculated across the entire batch before updating weights. If you are not using a &quot;minibatch&quot;, every training example in a &quot;batch&quot; updates the learning algorithm&#39;s parameters independently. . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([17, 18, 10, 22, 8, 14]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;, &#39;o&#39;)), (tensor([20, 15, 9, 13, 21, 12]), (&#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;, &#39;v&#39;, &#39;m&#39;)), (tensor([ 7, 25, 6, 5, 11, 23]), (&#39;h&#39;, &#39;z&#39;, &#39;g&#39;, &#39;f&#39;, &#39;l&#39;, &#39;x&#39;)), (tensor([ 1, 3, 0, 24, 19, 16]), (&#39;b&#39;, &#39;d&#39;, &#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;)), (tensor([2, 4]), (&#39;c&#39;, &#39;e&#39;))] . Putting It All Together . First, let&#39;s re-initialize our parameters: . weights = init_params((28*28,1)) bias = init_params(1) . A DataLoader can be created from a Dataset and we can set our bacth size here: . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . We&#39;ll do the same for the validation set: . valid_dl = DataLoader(valid_dset, batch_size=256) . Let&#39;s create a mini-batch of size 4 for testing: . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[-2.1876], [-8.3973], [ 2.5000], [-4.9473]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.7419, grad_fn=&lt;MeanBackward0&gt;) . Now we can calculate the gradients: . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0061), tensor([-0.0420])) . Let&#39;s put this into a function: . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0121), tensor([-0.0840])) . But look what happens if we call it twice: . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0182), tensor([-0.1260])) . The gradients have changed! The reason for this is that loss.backward actually adds the gradients of loss to any gradients that are currently stored. So, we have to set the current gradients to 0 first: . weights.grad.zero_() bias.grad.zero_(); . Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects in place. For instance, bias.zero_() sets all elements of the tensor bias to 0. . Our only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! . If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here&#39;s our basic training loop for an epoch: . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . We also want to check how we&#39;re doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it&#39;s greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with: . (preds&gt;0.0).float() == train_y[:4] . tensor([[False], [False], [ True], [False]]) . That gives us this function to calculate our validation accuracy: . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(0.2500) . and then put the batches together: . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5263 . That&#39;s our starting point. Let&#39;s train for one epoch, and see if the accuracy improves: . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.6663 . Then do a few more: . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8265 0.89 0.9183 0.9276 0.9398 0.9467 0.9506 0.9525 0.9559 0.9579 0.9598 0.9608 0.9613 0.9618 0.9633 0.9637 0.9647 0.9657 0.9672 0.9677 . Looking good! We&#39;re already about at the same accuracy as our &quot;pixel similarity&quot; approach, and we&#39;ve created a general-purpose foundation we can build on. Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it&#39;s called an optimizer. . Creating an Optimizer . PyTorch&#39;s nn.Linear does the same thing as our init_params and linear together. It contains both the weights and biases in a single class. Here&#39;s how we replicate our model from the previous section: . linear_model = nn.Linear(28*28,1) . Every PyTorch module knows what parameters it has that can be trained; they are available through the parameters method: . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . Now, let&#39;s create an optimizer: . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . We can create our optimizer by passing in the model&#39;s parameters: . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.4606 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7685 0.8554 0.9136 0.9346 0.9482 0.957 0.9634 0.9658 0.9678 0.9697 0.9717 0.9736 0.9746 0.9761 0.977 0.9775 0.9775 0.978 0.9785 . fastai provides the SGD class which, by default, does the same thing as our BasicOptim: . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.8179 0.8496 0.914 0.9346 0.9482 0.957 0.9619 0.9658 0.9673 0.9692 0.9712 0.9741 0.9751 0.9761 0.9775 0.9775 0.978 0.9785 0.979 . fastai also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders: . dls = DataLoaders(dl, valid_dl) . To create a Learner without using an application (such as cnn_learner) we need to pass in all the elements that we&#39;ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print: . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636709 | 0.503144 | 0.495584 | 00:00 | . 1 | 0.429828 | 0.248517 | 0.777233 | 00:00 | . 2 | 0.161680 | 0.155361 | 0.861629 | 00:00 | . 3 | 0.072948 | 0.097721 | 0.917566 | 00:00 | . 4 | 0.040128 | 0.073205 | 0.936212 | 00:00 | . 5 | 0.027210 | 0.059466 | 0.950442 | 00:00 | . 6 | 0.021837 | 0.050799 | 0.957802 | 00:00 | . 7 | 0.019398 | 0.044980 | 0.964181 | 00:00 | . 8 | 0.018122 | 0.040853 | 0.966143 | 00:00 | . 9 | 0.017330 | 0.037788 | 0.968106 | 00:00 | . Adding a Nonlinearity . So far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers—this is what gives us a neural network. . Here is the entire definition of a basic neural network: . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . That&#39;s it! All we have in simple_net is two linear classifiers with a max function between them. . Here, w1 and w2 are weight tensors, and b1 and b2 are bias tensors; that is, parameters that are initially randomly initialized, just like we did in the previous section: . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . The key point about this is that w1 has 30 output activations (which means that w2 must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that 30 to anything you like, to make the model more or less complex. . That little function res.max(tensor(0.0)) is called a rectified linear unit, also known as ReLU. We think we can all agree that rectified linear unit sounds pretty fancy and complicated... But actually, there&#39;s nothing more to it than res.max(tensor(0.0))—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as F.relu: . plot_function(F.relu) . Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier. . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . nn.Sequential creates a module that will call each of the listed layers or functions in turn. . nn.ReLU is a PyTorch module that does exactly the same thing as the F.relu function. Most functions that can appear in a model also have identical forms that are modules. Generally, it&#39;s just a case of replacing F with nn and changing the capitalization. When using nn.Sequential, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see nn.ReLU() in this example. . Because nn.Sequential is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let&#39;s try it out! As this is a deeper model, we&#39;ll use a lower learning rate and a few more epochs. . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.333021 | 0.396112 | 0.512267 | 00:00 | . 1 | 0.152461 | 0.235238 | 0.797350 | 00:00 | . 2 | 0.083573 | 0.117471 | 0.911678 | 00:00 | . 3 | 0.054309 | 0.078720 | 0.940628 | 00:00 | . 4 | 0.040829 | 0.061228 | 0.956330 | 00:00 | . 5 | 0.034006 | 0.051490 | 0.963690 | 00:00 | . 6 | 0.030123 | 0.045381 | 0.966634 | 00:00 | . 7 | 0.027619 | 0.041218 | 0.968106 | 00:00 | . 8 | 0.025825 | 0.038200 | 0.969087 | 00:00 | . 9 | 0.024441 | 0.035901 | 0.969578 | 00:00 | . 10 | 0.023321 | 0.034082 | 0.971541 | 00:00 | . 11 | 0.022387 | 0.032598 | 0.972031 | 00:00 | . 12 | 0.021592 | 0.031353 | 0.974485 | 00:00 | . 13 | 0.020904 | 0.030284 | 0.975466 | 00:00 | . 14 | 0.020300 | 0.029352 | 0.975466 | 00:00 | . 15 | 0.019766 | 0.028526 | 0.975466 | 00:00 | . 16 | 0.019288 | 0.027788 | 0.976448 | 00:00 | . 17 | 0.018857 | 0.027124 | 0.977429 | 00:00 | . 18 | 0.018465 | 0.026523 | 0.978410 | 00:00 | . 19 | 0.018107 | 0.025977 | 0.978901 | 00:00 | . 20 | 0.017777 | 0.025479 | 0.978901 | 00:00 | . 21 | 0.017473 | 0.025022 | 0.979392 | 00:00 | . 22 | 0.017191 | 0.024601 | 0.980373 | 00:00 | . 23 | 0.016927 | 0.024213 | 0.980373 | 00:00 | . 24 | 0.016680 | 0.023855 | 0.981354 | 00:00 | . 25 | 0.016449 | 0.023521 | 0.981354 | 00:00 | . 26 | 0.016230 | 0.023211 | 0.981354 | 00:00 | . 27 | 0.016023 | 0.022922 | 0.981354 | 00:00 | . 28 | 0.015827 | 0.022653 | 0.981845 | 00:00 | . 29 | 0.015641 | 0.022401 | 0.981845 | 00:00 | . 30 | 0.015463 | 0.022165 | 0.981845 | 00:00 | . 31 | 0.015294 | 0.021944 | 0.983317 | 00:00 | . 32 | 0.015132 | 0.021736 | 0.982826 | 00:00 | . 33 | 0.014977 | 0.021541 | 0.982826 | 00:00 | . 34 | 0.014828 | 0.021357 | 0.982336 | 00:00 | . 35 | 0.014686 | 0.021184 | 0.982336 | 00:00 | . 36 | 0.014549 | 0.021019 | 0.982336 | 00:00 | . 37 | 0.014417 | 0.020864 | 0.982336 | 00:00 | . 38 | 0.014290 | 0.020716 | 0.982336 | 00:00 | . 39 | 0.014168 | 0.020576 | 0.982336 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . And we can view the final accuracy: . learn.recorder.values[-1][2] . 0.98233562707901 . At this point we have something that is rather magical: . A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters A way to find the best set of parameters for any function (stochastic gradient descent) . Going Deeper . Here what happens when we train an 18-layer model . . . . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.128586 | 0.037928 | 0.996075 | 00:16 | . Almost 100% accuracy! . Jargon Recap . ReLU : Function that returns 0 for negative numbers and doesn&#39;t change positive numbers. | Mini-batch : A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch). | Forward pass : Applying the model to some input and computing the predictions. | Loss : A value that represents how well (or badly) our model is doing. | Gradient : The derivative of the loss with respect to some parameter of the model. | Backward pass : Computing the gradients of the loss with respect to all model parameters. | Gradient descent : Taking a step in the directions opposite to the gradients to make the model parameters a little bit better. | Learning rate : The size of the step we take when applying SGD to update the parameters of the model. | . Questionnaire . How is a grayscale image represented on a computer? How about a color image? . Images are represented by arrays with pixel values representing the content of the image. For greyscale images, a 2-dimensional array is used with the pixels representing the greyscale values, with a range of 256 integers. A value of 0 would represent white, and a value of 255 represents black, and different shades of greyscale in between. For color images, three color channels (red, green, blue) are typicall used, with a separate 256-range 2D array used for each channel. A pixel value of 0 again represents white, with 255 representing solid red, green, or blue. The three 2-D arrays form a final 3-D array (rank 3 tensor) representing the color image. . How are the files and folders in the MNIST_SAMPLE dataset structured? Why? . There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications. . Each subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit. . Explain how the &quot;pixel similarity&quot; approach to classifying digits works. . In the “pixel similarity” approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3’s from images of 7’s. We define the archetypical 3 as the pixel-wise mean value of all 3’s in the training set. Analoguously for the 7’s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent. In order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7. . What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. . Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering. . lst_in = range(10) lst_out = [2*el for el in lst_in if el%2==1] # is equivalent to: lst_out = [] for el in lst_in: if el%2==1: lst_out.append(2*el) . What is a &quot;rank-3 tensor&quot;? . The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3. Note that the term “rank” has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors). . What is the difference between tensor rank and shape? How do you get the rank from the shape? . Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. . How do you get the rank from the shape? . The length of a tensor’s shape is its rank. . So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this. . stacked_threes.shape . torch.Size([6131, 28, 28]) . We just need to find its length to know its rank. This is done as follows. . len(stacked_threes.shape) . 3 . You can also get a tensor’s rank directly with ndim . . stacked_threes.ndim . 3 . What are RMSE and L1 norm? . Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring “distance”. Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances. The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring). . How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? . As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python. Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done. . Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. . a = torch.Tensor(list(range(1,10))).view(3,3); print(a) . tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) . b = 2*a; print(b) . tensor([[ 2., 4., 6.], [ 8., 10., 12.], [14., 16., 18.]]) . b[1:,1:] . tensor([[10., 12.], [16., 18.]]) . What is broadcasting? . Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank. . Are metrics generally calculated using the training set, or the validation set? Why? . Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data. . What is SGD? . SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does. . Why does SGD use mini-batches? . We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU. . What are the seven steps in SGD for machine learning? . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | . How do we initialize the weights in a model? . Random weights work pretty well. . What is &quot;loss&quot;? . The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions. . Why can&#39;t we always use a high learning rate? . The loss may “bounce” around (oscillate) or even diverge, as the optimizer is taking steps that are too large, and updating the parameters faster than it should be. . What is a &quot;gradient&quot;? . The gradients tell us how much we have to change each weight to make our model better. It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative). . Do you need to know how to calculate gradients yourself? . Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you. This feature is known as automatic differentiation. In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward() . Why can&#39;t we use accuracy as a loss function? . A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn from the gradients equal to zero, and the model’s weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better predictions. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction. The loss function needs to reflect that. . Draw the sigmoid function. What is special about its shape? . . Sigmoid function is a smooth curve that squishes all values into values between 0 and 1. Most loss functions assume that the model is outputting some form of a probability or confidence level between 0 and 1 so we use a sigmoid function at the end of the model in order to do this. . What is the difference between a loss function and a metric? . The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model. . What is the function to calculate new weights using a learning rate? . The optimizer step function . What does the DataLoader class do? . The DataLoader class can take any Python collection and turn it into an iterator over many batches. . Write pseudocode showing the basic steps taken in each epoch for SGD. . for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? . def func(a,b): return list(zip(a,b)) . This data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label. . What does view do in PyTorch? . It changes the shape of a Tensor without changing its contents. . What are the &quot;bias&quot; parameters in a neural network? Why do we need them? . Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model. . What does the @ operator do in Python? . This is the matrix multiplication operator. . What does the backward method do? . This method returns the current gradients. . Why do we have to zero the gradients? . PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value. . What information do we have to pass to Learner? . We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print. . Show Python or pseudocode for the basic steps of a training loop. . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. . ReLU just means “replace any negative numbers with zero”. It is a commonly used activation function. . . What is an &quot;activation function&quot;? . The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem. . What&#39;s the difference between F.relu and nn.ReLU? . F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu. . The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? . There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements. . Further Research . Create your own implementation of Learner from scratch, based on the training loop shown in this chapter. | Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You&#39;ll need to do some of your own research to figure out how to overcome some obstacles you&#39;ll meet on the way. |",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/10/Tears-In-Rain.html",
            "relUrl": "/jupyter/2020/12/10/Tears-In-Rain.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Isn't it strange, to create something that hates you?",
            "content": "Data Ethics . This lesson we are looking at the Ethical implications of Data Science. Because there isn&#39;t any practical lesson, I will be leaving a link to the lecture here and filling in the questionnaire for future reference . Questionnaire . Does ethics provide a list of &quot;right answers&quot;? . There is no list of do’s and dont’s. Ethics is complicated, and context-dependent. It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. In this chapter, our goal is to provide some signposts to help you on that journey. . How can working with people of different backgrounds help when considering ethical questions? . Different people’s backgrounds will help them to see things which may not be obvious to you. Working with a team is helpful for many “muscle building” activities, including this one. . What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate? . IBM supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Hitler awarded Watson a special “Service to the Reich” medal in 1937. . But it also happened throughout the organization. IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on their punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM’s code for Jews in the concentration camps was 8, where around 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as “asocials”, with over 300,000 killed in the Zigeunerlager , or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6. . The marketers were just doing what they could to meet their business development goals. Edwin Black, author of “IBM and the Holocaust”, said: “To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM’s technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.” . What was the role of the first person jailed in the Volkswagen diesel scandal? . It was one of the engineers, James Liang, who just did what he was told. . What was the problem with a database of suspected gang members maintained by California law enforcement officials? . A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’d been added. . Why did YouTube&#39;s recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature? . The problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. . What are the problems with the centrality of metrics? . When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. . Why did Meetup.com not include gender in its recommendation system for tech meetups? . Evan Estola, lead machine learning engineer at Meetup, discussed the example of men expressing more interest than women in tech meetups. taking gender into account could therefore cause Meetup’s algorithm to recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop . What are the six types of bias in machine learning, according to Suresh and Guttag? . Historical Bias arises when there is a misalignment between world as it is and the values or objectives to be encoded and propagated in a model. It is a normative concern with the state of the world, and exists even given perfect sampling and feature selection. | Representation Bias arises while defining and sampling a development population. It occurs when the development population under-represents, and subsequently fails to generalize well, for some part of the use population. | Measurement Bias arises when choosing and measuring features and labels to use; these are often proxies for the desired quantities. The chosen set of features and labels may leave out important factors or introduce groupor input-dependent noise that leads to differential performance. | Aggregation Bias arises during model construction, when distinct populations are inappropriately combined. In many applications, the population of interest is heterogeneous and a single model is unlikely to suit all subgroups. | Evaluation Bias occurs during model iteration and evaluation. It can arise when the testing or external benchmark populations do not equally represent the various parts of the use population. Evaluation bias can also arise from the use of performance metrics that are not appropriate for the way in which the model will be used. | Deployment Bias occurs after model deployment, when a system is used or interpreted in inapppropriate ways. | Give two examples of historical race bias in the US. . When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients. | When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions. | Responding to apartment rental ads on Craigslist with a Black name elicited fewer responses than with a white name. | An all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had one Black member it convicted both at the same rate. | . Where are most images in ImageNet from? . The vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures . In the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot; why is sinusitis found to be predictive of a stroke? . We haven’t really measured stroke, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who had symptoms, went to a doctor, got the appropriate tests, and received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list—it&#39;s also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn&#39;t experience racial or gender-based medical discrimination, and more) . What is representation bias? . Representation Bias arises while defining and sampling a development population. It occurs when the development population under-represents, and subsequently fails to generalize well, for some part of the use population. . How are machines and people different, in terms of their use for making decisions? . Machines and the Neural Networks we build with them only have the context we provide from the Data, this Data can be incomplete or lack required contextual information . Is disinformation the same as &quot;fake news&quot;? . Some people think disinformation is primarily about false information or fake news, but in reality, disinformation can often contain seeds of truth, or half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the US and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. In The KGB and Soviet Disinformation (Pergamon) he wrote, &quot;Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, and deliberate lies.&quot; . Why is disinformation through auto-generated text a particularly significant issue? . Disinformation through autogenerated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we delve into creating language models . What are the five ethical lenses described by the Markkula Center? . The rights approach: Which option best respects the rights of all who have a stake? | The justice approach: Which option treats people equally or proportionately? | The utilitarian approach: Which option will produce the most good and do the least harm? | The common good approach: Which option best serves the community as a whole, not just some members? | The virtue approach: Which option leads me to act as the sort of person I want to be? | . Where is policy an appropriate tool for addressing data ethics issues? . Policies are an appropriate tool for addressing data ethics issues when is likely that design fixes, self regulation and technical approaches to addressing problems, involving ethical uses of Machine Learning are not working. . While such measures can be useful, they will not be sufficient to address the underlying problems that have led to our current state. For example, as long as it is incredibly profitable to create addictive technology, companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories and polluting our information ecosystem. While individual designers may try to tweak product designs, we will not see substantial changes until the underlying profit incentives changes. . Because of the above it is almost certain that policies will have to be created by government to address these issues. . Further Research: . Read the article &quot;What Happens When an Algorithm Cuts Your Healthcare&quot;. How could problems like this be avoided in the future? | Research to find out more about YouTube&#39;s recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government? | Read the paper &quot;Discrimination in Online Ad Delivery&quot;. Do you think Google should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response? | How can a cross-disciplinary team help avoid negative consequences? | Read the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot;. What actions do you think should be taken to deal with the issues identified in this paper? | Read the article &quot;How Will We Prevent AI-Based Forgery?&quot; Do you think Etzioni&#39;s proposed approach could work? Why? | Complete the section &quot;Analyze a Project You Are Working On&quot; in this chapter. | Consider whether your team could be more diverse. If so, what approaches might help? | Deep Learning in Practice: That&#39;s a Wrap! . Make sure that you have completed the following steps: . Connect to one of the GPU Jupyter servers recommended on the book&#39;s website. :heavy_check_mark: | Run the first notebook yourself. :heavy_check_mark: | Upload an image that you find in the first notebook; then try a few different images of different kinds to see what happens. :heavy_check_mark: | Run the second notebook, collecting your own dataset based on image search queries that you come up with. :heavy_check_mark: | Think about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice. :heavy_check_mark: | .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/07/Isnt_It_Strange.html",
            "relUrl": "/jupyter/2020/12/07/Isnt_It_Strange.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "These aren't the droids you're looking for . . .",
            "content": "From Model to Production . In this chapter we are going to make a model that can identify either C-3PO, R2-D2 or BB-8 (Just to keep the theme of the A.I in film theme going) and to see if this neural network could outsmart Obi Wan Kenobi and his use of the Force. We are going to create a seperate GitHub repo with a simple Juypter Notebook along with our Neural Network that has been pickled ready for deployment. . . The Practice of Deep Learning . The State of Deep Learning . . Things to consider: . We have a number of applications that Deep Learning is particularly good at Vision, Text, Tabular and Recsys (Recommendation Systems). Tabular data like spreadsheets and database tables is an area where deep learning is not always the best choice, it&#39;s particularly good for things involving high cardinality variables (variables that have lots and lots of descrete levels like post codes or product ID). Deep Learning is great for those in particular. . For Text its great at things like classification and translation although it is not particularly good for conversation, that has been a huge disappointment for a lot of companies that have tried to create conversation bots. Deep Learning isn&#39;t particularly good at providing accurate information, it is good at things that sound accurate or sound compelling but we don&#39;t have great ways of actually making sure that it is correct. . One big issue for reccomendation system collaborative filtering is that Deep Learning is focused on making predictions which don&#39;t necessarily mean creating useful recommendations. We will look into that later on. . Deep Learning is also really good at Multi-Modal that means things where you&#39;ve got multiple different types of data, so you might have some Tabular data including a Text column and an Image, then some Collaborative Filtering data and combining that all together is something that Deep Learning is really good at. So for example putting captions on photos is something Deep Learning is pretty good at, although, again, it&#39;s not very good at being accurate. . Ex. It might say an image contains two birds when in fact it is an image of three birds . The Drivetrain Approach . As mentioned above Recsys predictions are different from geniune reccomendations. . Recommendation engines are a familiar example of a data product based on well-built predictive models that do not achieve an optimal objective. The current algorithms predict what products a customer will like, based on purchase history and the histories of similar customers. A company like Amazon represents every purchase that has ever been made as a giant sparse matrix, with customers as the rows and products as the columns. Once they have the data in this format, data scientists apply some form of collaborative filtering to “fill in the matrix.” For example, if customer A buys products 1 and 10, and customer B buys products 1, 2, 4, and 10, the engine will recommend that A buy 2 and 4. These models are good at predicting whether a customer will like a given product, but they often suggest products that the customer already knows about or has already decided not to buy. Amazon’s recommendation engine is probably the best one out there, but it’s easy to get it to show its warts. Here is a screenshot of the “Customers Who Bought This Item Also Bought” feed on Amazon from a search for the latest book in Terry Pratchett’s “Discworld series:” . See article here . . Define an objective | Understand the levers: what inputs can you control? | What data can you collect? | Model the levers in order to understand how they affect the objective. | . Gathering Data . Now that we have defined our objective: . Find the droids! . We should now collect some data about what the droids look like. Having set up a Microsoft Azure account, and even posted about where you can find the Image Search Api key in my first blog post, having tested the search capabilities of Microsoft Bing I was in fact sorely dissappointed. . . If the internet was unable to find the droids I was looking for, how on earth was I going to train a model that could?! Luckily the Open Source community and the search_images_ddg function was able to do a much better job. Thanks Duck Duck Go! . For future reference, there is a cheeky way of downloading images from a Google Image Search in this great Juypter notebook. . search_images_ddg . &lt;function fastbook.search_images_ddg(term, max_images=200)&gt; . results = search_images_ddg(&#39;c3po&#39;, max_images=150) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 188 . dest = &#39;../images/c3po.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . droid_types = &#39;c3po&#39;,&#39;r2d2&#39;,&#39;bb8&#39; path = Path(&#39;droids&#39;) . if not path.exists(): path.mkdir() . for o in droid_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_ddg(f&#39;{o} robot&#39;, max_images=150) for i, result in enumerate(results): try: download_url(result, (dest/f&#39;{str(i).zfill(8)}.jpg&#39;)) except: pass . fns = get_image_files(path) fns . (#517) [Path(&#39;droids/c3po/00000000.jpg&#39;),Path(&#39;droids/c3po/00000001.jpg&#39;),Path(&#39;droids/c3po/00000002.jpg&#39;),Path(&#39;droids/c3po/00000003.jpg&#39;),Path(&#39;droids/c3po/00000004.jpg&#39;),Path(&#39;droids/c3po/00000005.jpg&#39;),Path(&#39;droids/c3po/00000006.jpg&#39;),Path(&#39;droids/c3po/00000007.jpg&#39;),Path(&#39;droids/c3po/00000008.jpg&#39;),Path(&#39;droids/c3po/00000009.jpg&#39;)...] . failed = verify_images(fns) failed . (#69) [Path(&#39;droids/c3po/00000010.jpg&#39;),Path(&#39;droids/c3po/00000011.jpg&#39;),Path(&#39;droids/c3po/00000014.jpg&#39;),Path(&#39;droids/c3po/00000022.jpg&#39;),Path(&#39;droids/c3po/00000036.jpg&#39;),Path(&#39;droids/c3po/00000043.jpg&#39;),Path(&#39;droids/c3po/00000047.jpg&#39;),Path(&#39;droids/c3po/00000048.jpg&#39;),Path(&#39;droids/c3po/00000049.jpg&#39;),Path(&#39;droids/c3po/00000050.jpg&#39;)...] . failed.map(Path.unlink); . From Data to DataLoaders . droids = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = droids.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . droids = droids.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = droids.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . droids = droids.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = droids.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . droids = droids.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = droids.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . droids = droids.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = droids.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . droids = droids.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = droids.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.321286 | 0.495739 | 0.168539 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.364726 | 0.363366 | 0.101124 | 00:09 | . 1 | 0.305098 | 0.292085 | 0.067416 | 00:08 | . 2 | 0.270196 | 0.287403 | 0.056180 | 00:08 | . 3 | 0.240431 | 0.287775 | 0.067416 | 00:08 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;../images/c3po.jpg&#39;) . (&#39;c3po&#39;, tensor(1), tensor([8.2749e-08, 1.0000e+00, 2.3360e-09])) . learn_inf.dls.vocab . [&#39;bb8&#39;, &#39;c3po&#39;, &#39;r2d2&#39;] . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your droid!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . . GitHub Binder . Questionnaire . Provide an example of where the droid classification model might work poorly in production, due to structural or style differences in the training data. . There are many cases that the droid classification model could fail, especially if these cases were not represented in the training data: . The droid is partially obstructed | Nighttime images are passed into the model | Low-resolution images are passed into the model | The droid is far away from the camera | The droid training dataset is highly biased towards one type of features (eg. color) | . Where do text models currently have a major deficiency? . Text models can generate context-appropriate text (like replies or imitating author style). However, text models still struggle with correct responses. Given factual information (such as a knowledge base), it is still hard to generate responses that utilizes this information to generate factually correct responses, though the text can seem very compelling. This can be very dangerous, as the layman may not be able to evaluate the factual accuracy of the generated text. . What are possible negative societal implications of text generation models? . The ability for text generation models to generate context-aware, highly compelling responses can be used at a massive scale to spread disinformation (“fake news”) and encourage conflict. . Models reinforce bias (like gender bias, racial bias) in training data and create a vicious cycle of biased outputs. . In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? . The predictions of the model could be reviewed by human experts for them to evaluate the results and determine what is the best next step. This is especially true for applying machine learning for medical diagnoses. For example, a machine learning model for identifying strokes in CT scans can alert high priority cases for expedited review, while other cases are still sent to radiologists for review. Or other models can also augment the medical professional’s abilities, reducing risk but still improving efficiency of the workflow. For example, deep learning models can provide useful measurements for radiologists or pathologists. . What kind of tabular data is deep learning particularly good at? . Deep learning is good at analyzing tabular data that includes natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code). . What&#39;s a key downside of directly using a deep learning model for recommendation systems? . Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example, if a user is familiar with other books from the same author, it isn’t helpful to recommend those products even though the user bought the author’s book. Or, recommending products a user may have already purchased. . What are the steps of the Drivetrain Approach? . . How do the steps of the Drivetrain Approach map to a recommendation system? . The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales . This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don’t have the information you need to actually optimize recommendations based on your true objective (more sales!) . Create an image recognition model using data you curate, and deploy it on the web. . See above . What is DataLoaders? . The DataLoaders class is the class that passes the data to the fastai model. It is essentially a class that stores the required Dataloader objects (usually for train and validation sets). . What four things do we need to tell fastai to create DataLoaders? . what kinds of data we are working with | how to get the list of items | how to label these items | how to create the validation set | . What does the splitter parameter to DataBlock do? . In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai’s predefined RandomSplitter class, providing it with the proportion of the data used for validation. . How do we ensure a random split always gives the same validation set? . It turns out it is impossible for our computers to generate truly random numbers. Instead, they use a process known as a pseudo-random generator. However, this process can be controlled using a random seed. By setting a random seed value, the pseudo-random generator will generate the “random” numbers in a fixed manner and it will be the same for every run. Using a random seed, we can generate a random split that gives the same validation set always. . What letters are often used to signify the independent and dependent variables? . x is independent. y is dependent. . What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? . crop is the default Resize() method, and it crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. For instance, if we were trying to recognize the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. . pad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images). If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use. . squish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. . Which resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful. . Another better method is RandomResizedCrop, in which we crop on a randomly selected region of the image. So every epoch, the model will see a different part of the image and will learn accordingly. . What is data augmentation? Why is it needed? . Data augmentation refers to creating random variations of our input data, such that they appear different, but not so different that it changes the meaning of the data. Examples include flipping, rotation, perspective warping, brightness changes, etc. Data augmentation is useful for the model to better understand the basic concept of what an object is and how the objects of interest are represented in images. Therefore, data augmentation allows machine learning models to generalize . This is especially important when it can be slow and expensive to label data. . What is the difference between item_tfms and batch_tfms? . item_tfms are transformations applied to a single data sample x on the CPU. Resize() is a common transform because the mini-batch of input images to a cnn must have the same dimensions. Assuming the images are RGB with 3 channels, then Resize() as item_tfms will make sure the images have the same width and height. . batch_tfms are applied to batched data samples (aka individual samples that have been collated into a mini-batch) on the GPU. They are faster and more efficient than item_tfms. A good example of these are the ones provided by aug_transforms(). Inside are several batch-level augmentations that help many models. . What is a confusion matrix? . A class confusion matrix is a representation of the predictions made vs the correct labels. The rows of the matrix represent the actual labels while the columns represent the predictions. Therefore, the number of images in the diagonal elements represent the number of correctly classified images, while the off-diagonal elements are incorrectly classified images. Class confusion matrices provide useful information about how well the model is doing and which classes the model might be confusing . . What does export save? . export saves both the architecture, as well as the trained parameters of the neural network architecture. It also saves how the DataLoaders are defined. . What is it called when we use a model for getting predictions, instead of training? . Inference . What are IPython widgets? . IPython widgets are JavaScript and Python combined functionalities that let us build and interact with GUI components directly in a Jupyter notebook. An example of this would be an upload button, which can be created with the Python function widgets.FileUpload(). . When might you want to use CPU for deployment? When might GPU be better? . GPUs are best for doing identical work in parallel. If you will be analyzing single pieces of data at a time (like a single image or single sentence), then CPUs may be more cost effective instead, especially with more market competition for CPU servers versus GPU servers. GPUs could be used if you collect user responses into a batch at a time, and perform inference on the batch. This may require the user to wait for model predictions. Additionally, there are many other complexities when it comes to GPU inference, like memory management and queuing of the batches. . What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? . The application will require network connection, and there will be extra network latency time when submitting input and returning results. Additionally, sending private data to a network server can lead to security concerns. . What are three examples of problems that could occur when rolling out a bear warning system in practice? . The model we trained will likely perform poorly when: . Handling night-time images | Dealing with low-resolution images (ex: some smartphone images) | The model returns prediction too slowly to be useful | . What is &quot;out-of-domain data&quot;? . Data that is fundamentally different in some aspect compared to the model’s training data. For example, an object detector that was trained exclusively with outside daytime photos is given a photo taken at night. . What is &quot;domain shift&quot;? . This is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data. . What are the three steps in the deployment process? . Manual process – the model is run in parallel and not directly driving any actions, with humans still checking the model outputs. | Limited scope deployment – The model’s scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised. | Gradual expansion – The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better). | . Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/03/These-Arent-The-Droids.html",
            "relUrl": "/jupyter/2020/12/03/These-Arent-The-Droids.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Greetings, programs!",
            "content": "My Strategy . The incredible team at fast.ai have selflessly opened the source of their book Deep Learning for Coders with fastai &amp; Pytorch along with stripped down juypter notebooks to aid with the students understanding of the course work. By utilising these stripped down notebooks, running them on my notebook server of choice, Paperspace Gradient, reading the code and understanding the output, I can then download them and fill in my notes as the basis of my blog post. This is exactly what I am doing today. This way I will have comprehensive notes for each chapter available in one place for easy reference. . The Software: PyTorch, fastai, and Jupyter . The Software Stack . . fastai is a high level api build on top of PyTorch which in turn is built on top of Python, which means that what you can do in PyTorch you can do in fastai. This is incredibly powerful, it allows us as developers to work in as highly abstracted manner as we like, opening the door for people who want to use these tools at which ever level they feel comfortable. . Lets look a couple of example of how this translates: . with torch.no_grad(): model.eval() for batch in dl: preds = model(batch) . Let&#39;s say, for example, I was trying to get some predictions across an entire data loader, which could be a series of images. In PyTorch the inferance loop would look something like the code above, in contrast: . learn.get_preds(dl=dl) . fastai has a very simple get_preds function . Lets say I want to deploy my model, and so my inference code for PyTorch might look something like this: . im = Image.open(image_name) c = transforms.Resize(224) t = transforms.ToTensor() n = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) loader = transforms.Compose([c, t, n]) im = load(im).float() im = Variable(im, requires_grad=True) im = im.unsqueeze(0) im = im.cuda() . Here I prepare my transfroms, I compose them all together into a pipeline, pass my image through, then convert it to a tensor, then move it onto the GPU, then eventually I can call my model and repeat what we did above. In fastai it&#39;s trivial: . learn.predict(image_name) . This will automatically do all of the above for us. The PyTorch code although realtively straightforward and procedural can be intimidating for new users, there are a number of steps that are required to get things working. With fastai this process is democratized and allows people to start working with these tools far faster. . A Layered API . . fastai has what it calls a Layered API, what that means is that every aspect of the data processing pipeline can be fully customizable. To sum it up in five steps: . You would first efine what your datablocks are and these blocks would determine your input and your outputs. So if I was doing a classification problem with images, I might chosse and image block for my inputs and a catagory block for my outputs since I want to predict a catagory. | You would then tell fastai how I would like to get my data | Split that data | How you would like to label it | Then creating a DataLoader object that has all the data inside of it, along with any image augmentations that might happen and any normalizations | So lets look a few examples: . Here is what a segmentation DataBlock might look like: . seg_dls = DataBlock(blocks=(ImageBlock, MaskBlock), get_items=get_image_files, get_y=get_y, splitter=RandomSplitter(), batch_tfms=batch_tfms) . Here is what a GAN DataBlock might look like: . gan_dls = DataBlock(blocks=(ImageBlock, ImageBlock), get_items=get_image_files, get_y=get_y, splitter=RandomSplitter(), item_tfms=item_tfms, batch_tfms=batch_tfms) . Here is what a text classification DataBlock might look like: . txt_dls = DataBlock(blocks=(TextBlock.from_df(&#39;text&#39;), CatagoryBlock()), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;label&#39;), splitter=ColSplitter()) . As you can see from an API standpoint our interface to the Layered API is very simple and intuitive, allowing us a unified interface no matter what the type of problem we are trying to solve. Ultimately we are defining our input and output data types, telling fastai how we are going to get our data, how that data will be split up and labeled and perform any transforms or augmentations on that data ready for training. . Following this approach we can fit almost any problem into this nice and easy to follow framework. Let&#39;s check out my first model in depth. . My First Model . fastai&#39;s top down teaching methodology gives you the keys to car straight away, showing you what is possible with deep learning frameworks and get&#39;s upi producing world class models in a very short period of time. . So how does fastai successfully simplify the entire training pipeline? . My first model can tell if an image is either a cat or a dog with 99% accuracy in only 10 lines of code! (And we can have it ready for deployment by adding 1 more line, but more about that in Chapter 2). What is even more incredible is that it took only a few minutes to train! This is the code in it&#39;s entirety below: . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.161289 | 0.021331 | 0.009472 | 00:30 | . epoch train_loss valid_loss error_rate time . 0 | 0.067894 | 0.041609 | 0.010149 | 00:36 | . Why so much Magic? . In some cases magic and obfuscation can be pretty bad, so why abstract the process like this? Through intensive experimentation, the fastai team have been able to find some pretty good default parameters that give good results on most tasks. This includes the learning rate, the hyper parameters to set for your optimizer and what this allows is that you can go from start-to-finish with a pretty wide range of standard problems without a whole lot of heavy lifting . . Most libraries that decide to lean into using magic / obfuscation tend to try and keep you away from the implementation, but fastai with its layered API approach you can remove more and more of the layers of obfuscation the further down the pipeline you go. To that end you can use a raw PyTorch data loader and a model, since fastai models are PyTorch models and you can simply use fastai as a training framework. . Here are some great resources to keep in mind as you learn: . Zachary Mueller Notebooks . Walk with fastai . The fastai Forum . Testing the model . uploader = widgets.FileUpload() uploader . . img = PILImage.create(uploader.data[0]) is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . Is this a cat?: False. Probability it&#39;s a cat: 0.000047 . Limitations Inherent To Machine Learning . A model cannot be created without data. | A model can only learn to operate on the patterns seen in the input data used to train it. | This learning approach only creates predictions, not recommended actions. | It&#39;s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren&#39;t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats). | . Deep Learning Is Not Just for Image Classification . path = untar_data(URLs.CAMVID_TINY) dls = SegmentationDataLoaders.from_label_func( path, bs=8, fnames = get_image_files(path/&quot;images&quot;), label_func = lambda o: path/&#39;labels&#39;/f&#39;{o.stem}_P{o.suffix}&#39;, codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) ) learn = unet_learner(dls, resnet34) learn.fine_tune(8) . epoch train_loss valid_loss time . 0 | 2.637404 | 2.316639 | 00:02 | . epoch train_loss valid_loss time . 0 | 1.773122 | 1.583480 | 00:02 | . 1 | 1.544705 | 1.252534 | 00:02 | . 2 | 1.398063 | 1.001754 | 00:02 | . 3 | 1.248483 | 0.803304 | 00:02 | . 4 | 1.113762 | 0.768532 | 00:02 | . 5 | 1.003168 | 0.729955 | 00:02 | . 6 | 0.912422 | 0.711288 | 00:02 | . 7 | 0.839843 | 0.701163 | 00:02 | . learn.show_results(max_n=6, figsize=(7,8)) . from fastai.tabular.all import * path = untar_data(URLs.ADULT_SAMPLE) dls = TabularDataLoaders.from_csv(path/&#39;adult.csv&#39;, path=path, y_names=&quot;salary&quot;, cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;], cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;], procs = [Categorify, FillMissing, Normalize]) learn = tabular_learner(dls, metrics=accuracy) . learn.fit_one_cycle(3) . epoch train_loss valid_loss accuracy time . 0 | 0.364839 | 0.359383 | 0.835227 | 00:05 | . 1 | 0.353714 | 0.349704 | 0.836916 | 00:05 | . 2 | 0.345267 | 0.347448 | 0.838298 | 00:05 | . from fastai.collab import * path = untar_data(URLs.ML_SAMPLE) dls = CollabDataLoaders.from_csv(path/&#39;ratings.csv&#39;) learn = collab_learner(dls, y_range=(0.5,5.5)) learn.fine_tune(10) . epoch train_loss valid_loss time . 0 | 1.523289 | 1.381516 | 00:00 | . epoch train_loss valid_loss time . 0 | 1.396122 | 1.319708 | 00:00 | . 1 | 1.277794 | 1.132567 | 00:00 | . 2 | 1.022374 | 0.830664 | 00:00 | . 3 | 0.800652 | 0.694179 | 00:00 | . 4 | 0.700407 | 0.659416 | 00:00 | . 5 | 0.658882 | 0.647847 | 00:00 | . 6 | 0.638177 | 0.643430 | 00:00 | . 7 | 0.621591 | 0.641302 | 00:00 | . 8 | 0.607097 | 0.640517 | 00:00 | . 9 | 0.607002 | 0.640406 | 00:00 | . learn.show_results() . userId movieId rating rating_pred . 0 5.0 | 81.0 | 5.0 | 4.451847 | . 1 68.0 | 76.0 | 2.0 | 3.938959 | . 2 88.0 | 97.0 | 3.5 | 3.962249 | . 3 55.0 | 78.0 | 2.5 | 3.165076 | . 4 34.0 | 67.0 | 5.0 | 4.236149 | . 5 39.0 | 55.0 | 4.0 | 4.179371 | . 6 5.0 | 44.0 | 4.0 | 3.103317 | . 7 13.0 | 57.0 | 4.0 | 4.266764 | . 8 16.0 | 33.0 | 4.0 | 3.128462 | . Questionnaire . Do you need these for deep learning? . Lots of math - False | Lots of data - False | Lots of expensive computers - False | A PhD - False | . Name five areas where deep learning is now the best in the world. . Any five of the following: . Natural Language Processing (NLP) – Question Answering, Document Summarization and Classification, etc. | Computer Vision – Satellite and drone imagery interpretation, face detection and recognition, image captioning, etc. | Medicine – Finding anomalies in medical images (ex: CT, X-ray, MRI), detecting features in tissue slides (pathology), diagnosing diabetic retinopathy, etc. | Biology – Folding proteins, classifying, genomics tasks, cell classification, etc. | Image generation/enhancement – colorizing images, improving image resolution (super-resolution), removing noise from images (denoising), converting images to art in style of famous artists (style transfer), etc. | Recommendation systems – web search, product recommendations, etc. | Playing games – Super-human performance in Chess, Go, Atari games, etc | Robotics – handling objects that are challenging to locate (e.g. transparent, shiny, lack of texture) or hard to pick up | Other applications – financial and logistical forecasting; text to speech; much much more. | . What was the name of the first device that was based on the principle of the artificial neuron? . Mark I perceptron built by Frank Rosenblatt . Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? . A set of processing units | A state of activation | An output function for each unit | A pattern of connectivity among units | A propagation rule for propagating patterns of activities through the network of connectivities | An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit | A learning rule whereby patterns of connectivity are modified by experience | An environment within which the system must operate | . What were the two theoretical misunderstandings that held back the field of neural networks? . In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, “Perceptrons”, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter. . In the 1980’s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential. . What is a GPU? . GPU stands for Graphics Processing Unit (also known as a graphics card). Standard computers have various components like CPUs, RAM, etc. CPUs, or central processing units, are the core units of all standard computers, and they execute the instructions that make up computer programs. GPUs, on the other hand, are specialized units meant for displaying graphics, especially the 3D graphics in modern computer games. The hardware optimizations used in GPUs allow it to handle thousands of tasks at the same time. Incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU. . Open a notebook and execute a cell containing: 1+1. What happens? . In a Jupyter Notebook, we can create code cells and run code in an interactive manner. When we execute a cell containing some code (in this case: 1+1), the code is run by Python and the output is displayed underneath the code cell (in this case: 2). . Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. . See Above . . . . Complete the Jupyter Notebook online appendix. . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. | Why is it hard to use a traditional computer program to recognize images in a photo? . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. | What did Samuel mean by &quot;weight assignment&quot;? . “weight assignment” refers to the current values of the model parameters. Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ” and a “ mechanism for altering the weight assignment so as to maximize the performance ”. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance. . What term do we normally use in deep learning for what Samuel called &quot;weights&quot;? . We instead use the term parameters. In deep learning, the term “weights” has a separate meaning. (The neural network has various parameters that we fit our data to. As shown in upcoming chapters, the two types of neural network parameters are weights and biases) . Draw a picture that summarizes Samuel&#39;s view of a machine learning model. . . Why is it hard to understand why a deep learning model makes a particular prediction? . This is a highly-researched topic known as interpretability of deep learning models. Deep learning models are hard to understand in part due to their “deep” nature. Think of a linear regression model. Simply, we have some input variables/data that are multiplied by some weights, giving us an output. We can understand which variables are more important and which are less important based on their weights. A similar logic might apply for a small neural network with 1-3 layers. However, deep neural networks have hundreds, if not thousands, of layers. It is hard to determine which factors are important in determining the final output. The neurons in the network interact with each other, with the outputs of some neurons feeding into other neurons. Altogether, due to the complex nature of deep learning models, it is very difficult to understand why a neural network makes a given prediction. . However, in some cases, recent research has made it easier to better understand a neural network’s prediction. For example, as shown in this chapter, we can analyze the sets of weights and determine what kind of features activate the neurons. When applying CNNs to images, we can also see which parts of the images highly activate the model. We will see how we can make our models interpretable later in the book. . What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . The universal approximation theorem states that neural networks can theoretically represent any mathematical function. However, it is important to realize that practically, due to the limits of available data and computer hardware, it is impossible to practically train a model to do so. But we can get very close! . What do you need in order to train a model? . You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer). . How could a feedback loop impact the rollout of a predictive policing model? . In a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop . Do we always have to use 224×224-pixel images with the cat recognition model? . No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption. . What is the difference between classification and regression? . Classification is focused on predicting a class or category (ex: type of pet). Regression is focused on predicting a numeric quantity (ex: age of pet). . What is a validation set? What is a test set? Why do we need them? . The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data. . What will fastai do if you don&#39;t provide a validation set? . fastai will automatically create a validation dataset. It will randomly take 20% of the data and assign it as the validation set ( valid_pct = 0.2 ). . Can we always use a random sample for a validation set? Why or why not? . A good validation or test set should be representative of new data you will see in the future. Sometimes this isn’t true if a random sample is used. For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach. . What is overfitting? Provide an example. . Overfitting is the most challenging issue when it comes to training machine learning models. Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data. This is especially important when it comes to neural networks, because neural networks can potentially “memorize” the dataset that the model was trained on, and will perform abysmally on unseen data because it didn’t “memorize” the ground truth values for that data. This is why a proper validation framework is needed by splitting the data into training, validation, and test sets. . What is a metric? How does it differ from &quot;loss&quot;? . A metric is a function that measures quality of the model’s predictions using the validation set. This is similar to the ­ loss , which is also a measure of performance of the model. However, loss is meant for the optimization algorithm (like SGD) to efficiently update the model parameters, while metrics are human-interpretable measures of performance. Sometimes, a metric may also be a good choice for the loss. . How can pretrained models help? . Pretrained models have been trained on other problems that may be quite similar to the current task. For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects. Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection. However, since the model was trained for a different task than already used, this model cannot be used as is. . What is the &quot;head&quot; of a model? . When using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model. . What kinds of features do the early layers of a CNN find? How about the later layers? . Earlier layers learn simple features like diagonal, horizontal, and vertical edges. Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals. . Are image models only useful for photos? . No! Image models can be useful for other types of images like sketches, medical data, etc. . However, a lot of information can be represented as images . For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio. Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification. There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem, and use pretrained image models to obtain state-of-the-art results! . What is an &quot;architecture&quot;? . The architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit. . What is segmentation? . At its core, segmentation is a pixelwise classification problem. We attempt to predict a label for every single pixel in the image. This provides a mask for which parts of the image correspond to the given label. . What is y_range used for? When do we need it? . y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5). . What are &quot;hyperparameters&quot;? . Training models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters. . What&#39;s the best way to avoid failures when using AI in an organization? . Key things to consider when using AI in an organization: . Make sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner. | Try out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases. | .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/02/Greetings-Programs.html",
            "relUrl": "/jupyter/2020/12/02/Greetings-Programs.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "HAL O World",
            "content": "Learning to Deep Learn . . . So today is a monumental day in the field of deep learning, AlphaFold by DeepMind has been recognised as a solution to the “protein folding problem” and marks the end of a challenge set over 50 years ago. This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world. So now is as good a time as any to really get involved and start learning to Deep Learn. . Deep Learning for Coders . To ensure that I am making good progress, creating practical models and work as effectiently as possible I will be working through the book &quot;Deep Learning for Coders with fastai and PyTorch&quot;, not only is there a great online portal with a video walk through of the curriculum, an incredible and very well presented book, but also all the source (not only for the practical work, but also for the entire book! It was all written in jupyter notebooks, how amazing is that!). . Getting Setup . So far I have made it through a couple of chapters of the book, and as reccomended by the course I am starting a &quot;Deep Learning Journal&quot;, just a place online that I can track my progress, thoughts and experiments as I progress through the curriculum. There are a couple of steps that are required to get setup to make the most of the work presented, and I thought that a good place to start a blog is with some notes on how to get setup. . Juypter Notebooks . Working as a Technical Animator in video games and having written Python code for nearly a decade now, I was aware of juypter notebooks, but I felt that as it was only useful for creating &quot;pretty comments&quot; inline with your code. Yes it would be cool to have an animated gif as part of your documentation but how useful are they in reality? It turns out that they are awesome! I am writing this very blog post using them right now and I am really impressed. The fastai team have created this entire blogging platform fastpages to help people get up and running quickly. Jeremy from fastai has also created a great talk about the power of notebooks and their platform nbdev here which is worth a watch. . Anaconda Navigator . Getting set up with juypter on Windows 10 is painless, there is an incredible tool called Anaconda Navaigator, an all in one data science toolkit for python that can also manage your packages and your environments through a simple interface. No more package dependency hell, especially when installing large packages for deep learning. Simply follow the link above and download the graphical installer for your OS. Once installed, you can launch your juypter notebook environment here: . . Fastpages . Fastpages the platform I am using to setup your blog is incredibly simple and the team at fast.ai have documented the process amazingly, simply stepping through the documentation will allow you to copy their directory as a template along with all of the GitHub actions that will build and display your blog. You can see the documentation here . . Paperspace Gradient . Another fantastic facet of the course is that they don&#39;t encourage you to wade into the murky world of systems administration setting up packages, environments and GPU compliant kernels to get you up and running working with state-of-the-art deep learning models. They instead encourage you to look into using a Server Notebook, they reccommend either Collab, Gradient or Sagemaker. I chose Paperspace Gradient and I am very happy with my choice. . Microsoft Azure . By far the most frustrating part of the set up for the course has nothing to do with Python, the notebook server, GitHub or fast.ai. In Chapter 2 of the book, the course encourages you to set up a Microsoft Azure account to get an API key so you can use their Image Search conginitive service to pull training images from the web to train a classifier model. Unfortunately the documentation on the site and the forum isn&#39;t incredibly helpful, so I am logging it here incase anyone should find it useful, but mainly if I have to do this again on another account I can remeber how and where to find everything. . Go to: https://azure.microsoft.com/en-gb/services/cognitive-services/ . . Scroll to the API&#39;s section of the page, and select &quot;Web Search&quot; . . This will redirect you to https://www.microsoft.com/en-us/bing/apis/bing-web-search-api . . Ensuring you already have an Azure account, this will take you to the console to create a Bing resource . . Once you have filled in the details and created your resource it will take you to your Azure dashboard . . Go to the &quot;Keys and Endpoint&quot; tab, this is where you can find your Bing Image Search Api Key . . Now that you have your API key and replaced the XXX value in the Chapter 2 notebook, there is once more piece of code required to get the code in Chapter to work, below is a re-write of the search_images_bing definition, simply paste it as a code cell above where it is being called in notebook. Everything should work for you now. . def search_images_bing(subscription_key, search_term, size = 150): search_url = &quot;https://api.bing.microsoft.com/v7.0/images/search&quot; headers = {&quot;Ocp-Apim-Subscription-Key&quot; : subscription_key} params = {&quot;q&quot;: search_term, &quot;license&quot;: &quot;public&quot;, &quot;imageType&quot;: &quot;photo&quot;, &quot;count&quot;: size} response = requests.get(search_url, headers=headers, params=params) response.raise_for_status() search_results = response.json() reformatted_results = L(search_results[&quot;value&quot;], use_list=True) # Uses the FastAI class L, a # &quot;drop-in&quot; replacement for Lists. Mixes Python standard library and numpy arrays. # I&#39;m putting this here so, again, we minimize the amount of individual cells rewritten. # Many of the later cells assume .attrgot is a valid thing you can call. for result in reformatted_results: result[&quot;content_url&quot;] = result[&quot;contentUrl&quot;] # Bing changed their API. They return contentUrl instead of content_url. # Again, this will help in the long run. return reformatted_results .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/01/HAL-O-World.html",
            "relUrl": "/jupyter/2020/12/01/HAL-O-World.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bailey-deep-learning.github.io/im_sorry_dave/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bailey-deep-learning.github.io/im_sorry_dave/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}