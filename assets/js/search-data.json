{
  
    
        "post0": {
            "title": "Greetings, programs!",
            "content": "My Strategy . The incredible team at fast.ai have selflessly opened the source of their book Deep Learning for Coders with fastai &amp; Pytorch along with stripped down juypter notebooks to aid with the students understanding of the course work. By utilising these stripped down notebooks, running them on my notebook server of choice, Paperspace Gradient, reading the code and understanding the output, I can then download them and fill in my notes as the basis of my blog post. This is exactly what I am doing today. This way I will have comprehensive notes for each chapter available in one place for easy reference. . The Software: PyTorch, fastai, and Jupyter . The Software Stack . . fastai is a high level api build on top of PyTorch which in turn is built on top of Python, which means that what you can do in PyTorch you can do in fastai. This is incredibly powerful, it allows us as developers to work in as highly abstracted manner as we like, opening the door for people who want to use these tools at which ever level they feel comfortable. . Lets look a couple of example of how this translates: . with torch.no_grad(): model.eval() for batch in dl: preds = model(batch) . Let&#39;s say, for example, I was trying to get some predictions across an entire data loader, which could be a series of images. In PyTorch the inferance loop would look something like the code above, in contrast: . learn.get_preds(dl=dl) . fastai has a very simple get_preds function . Lets say I want to deploy my model, and so my inference code for PyTorch might look something like this: . im = Image.open(image_name) c = transforms.Resize(224) t = transforms.ToTensor() n = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) loader = transforms.Compose([c, t, n]) im = load(im).float() im = Variable(im, requires_grad=True) im = im.unsqueeze(0) im = im.cuda() . Here I prepare my transfroms, I compose them all together into a pipeline, pass my image through, then convert it to a tensor, then move it onto the GPU, then eventually I can call my model and repeat what we did above. In fastai it&#39;s trivial: . learn.predict(image_name) . This will automatically do all of the above for us. The PyTorch code although realtively straightforward and procedural can be intimidating for new users, there are a number of steps that are required to get things working. With fastai this process is democratized and allows people to start working with these tools far faster. . A Layered API . . fastai has what it calls a Layered API, what that means is that every aspect of the data processing pipeline can be fully customizable. To sum it up in five steps: . You would first efine what your datablocks are and these blocks would determine your input and your outputs. So if I was doing a classification problem with images, I might chosse and image block for my inputs and a catagory block for my outputs since I want to predict a catagory. | You would then tell fastai how I would like to get my data | Split that data | How you would like to label it | Then creating a DataLoader object that has all the data inside of it, along with any image augmentations that might happen and any normalizations | So lets look a few examples: . Here is what a segmentation DataBlock might look like: . seg_dls = DataBlock(blocks=(ImageBlock, MaskBlock), get_items=get_image_files, get_y=get_y, splitter=RandomSplitter(), batch_tfms=batch_tfms) . Here is what a GAN DataBlock might look like: . gan_dls = DataBlock(blocks=(ImageBlock, ImageBlock), get_items=get_image_files, get_y=get_y, splitter=RandomSplitter(), item_tfms=item_tfms, batch_tfms=batch_tfms) . Here is what a text classification DataBlock might look like: . txt_dls = DataBlock(blocks=(TextBlock.from_df(&#39;text&#39;), CatagoryBlock()), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;label&#39;), splitter=ColSplitter()) . As you can see from an API standpoint our interface to the Layered API is very simple and intuitive, allowing us a unified interface no matter what the type of problem we are trying to solve. Ultimately we are defining our input and output data types, telling fastai how we are going to get our data, how that data will be split up and labeled and perform any transforms or augmentations on that data ready for training. . Following this approach we can fit almost any problem into this nice and easy to follow framework. Let&#39;s check out my first model in depth. . My First Model . fastai&#39;s top down teaching methodology gives you the keys to car straight away, showing you what is possible with deep learning frameworks and get&#39;s upi producing world class models in a very short period of time. . So how does fastai successfully simplify the entire training pipeline? . My first model can tell if an image is either a cat or a dog with 99% accuracy in only 10 lines of code! (And we can have it ready for deployment by adding 1 more line, but more about that in Chapter 2). What is even more incredible is that it took only a few minutes to train! This is the code in it&#39;s entirety below: . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.161289 | 0.021331 | 0.009472 | 00:30 | . epoch train_loss valid_loss error_rate time . 0 | 0.067894 | 0.041609 | 0.010149 | 00:36 | . Why so much Magic? . In some cases magic and obfuscation can be pretty bad, so why abstract the process like this? Through intensive experimentation, the fastai team have been able to find some pretty good default parameters that give good results on most tasks. This includes the learning rate, the hyper parameters to set for your optimizer and what this allows is that you can go from start-to-finish with a pretty wide range of standard problems without a whole lot of heavy lifting . . Most libraries that decide to lean into using magic / obfuscation tend to try and keep you away from the implementation, but fastai with its layered API approach you can remove more and more of the layers of obfuscation the further down the pipeline you go. To that end you can use a raw PyTorch data loader and a model, since fastai models are PyTorch models and you can simply use fastai as a training framework. . Here are some great resources to keep in mind as you learn: . Zachary Mueller Notebooks . Walk with fastai . The fastai Forum . Testing the model . uploader = widgets.FileUpload() uploader . . img = PILImage.create(uploader.data[0]) is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . Is this a cat?: False. Probability it&#39;s a cat: 0.000047 . Limitations Inherent To Machine Learning . From this picture we can now see some fundamental things about training a deep learning model: . A model cannot be created without data. | A model can only learn to operate on the patterns seen in the input data used to train it. | This learning approach only creates predictions, not recommended actions. | It&#39;s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren&#39;t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats). | . Deep Learning Is Not Just for Image Classification . path = untar_data(URLs.CAMVID_TINY) dls = SegmentationDataLoaders.from_label_func( path, bs=8, fnames = get_image_files(path/&quot;images&quot;), label_func = lambda o: path/&#39;labels&#39;/f&#39;{o.stem}_P{o.suffix}&#39;, codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) ) learn = unet_learner(dls, resnet34) learn.fine_tune(8) . epoch train_loss valid_loss time . 0 | 2.637404 | 2.316639 | 00:02 | . epoch train_loss valid_loss time . 0 | 1.773122 | 1.583480 | 00:02 | . 1 | 1.544705 | 1.252534 | 00:02 | . 2 | 1.398063 | 1.001754 | 00:02 | . 3 | 1.248483 | 0.803304 | 00:02 | . 4 | 1.113762 | 0.768532 | 00:02 | . 5 | 1.003168 | 0.729955 | 00:02 | . 6 | 0.912422 | 0.711288 | 00:02 | . 7 | 0.839843 | 0.701163 | 00:02 | . learn.show_results(max_n=6, figsize=(7,8)) . from fastai.tabular.all import * path = untar_data(URLs.ADULT_SAMPLE) dls = TabularDataLoaders.from_csv(path/&#39;adult.csv&#39;, path=path, y_names=&quot;salary&quot;, cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;], cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;], procs = [Categorify, FillMissing, Normalize]) learn = tabular_learner(dls, metrics=accuracy) . learn.fit_one_cycle(3) . epoch train_loss valid_loss accuracy time . 0 | 0.364839 | 0.359383 | 0.835227 | 00:05 | . 1 | 0.353714 | 0.349704 | 0.836916 | 00:05 | . 2 | 0.345267 | 0.347448 | 0.838298 | 00:05 | . from fastai.collab import * path = untar_data(URLs.ML_SAMPLE) dls = CollabDataLoaders.from_csv(path/&#39;ratings.csv&#39;) learn = collab_learner(dls, y_range=(0.5,5.5)) learn.fine_tune(10) . epoch train_loss valid_loss time . 0 | 1.523289 | 1.381516 | 00:00 | . epoch train_loss valid_loss time . 0 | 1.396122 | 1.319708 | 00:00 | . 1 | 1.277794 | 1.132567 | 00:00 | . 2 | 1.022374 | 0.830664 | 00:00 | . 3 | 0.800652 | 0.694179 | 00:00 | . 4 | 0.700407 | 0.659416 | 00:00 | . 5 | 0.658882 | 0.647847 | 00:00 | . 6 | 0.638177 | 0.643430 | 00:00 | . 7 | 0.621591 | 0.641302 | 00:00 | . 8 | 0.607097 | 0.640517 | 00:00 | . 9 | 0.607002 | 0.640406 | 00:00 | . learn.show_results() . userId movieId rating rating_pred . 0 5.0 | 81.0 | 5.0 | 4.451847 | . 1 68.0 | 76.0 | 2.0 | 3.938959 | . 2 88.0 | 97.0 | 3.5 | 3.962249 | . 3 55.0 | 78.0 | 2.5 | 3.165076 | . 4 34.0 | 67.0 | 5.0 | 4.236149 | . 5 39.0 | 55.0 | 4.0 | 4.179371 | . 6 5.0 | 44.0 | 4.0 | 3.103317 | . 7 13.0 | 57.0 | 4.0 | 4.266764 | . 8 16.0 | 33.0 | 4.0 | 3.128462 | . Validation Sets and Test Sets . Use Judgment in Defining Test Sets . Questionnaire . Do you need these for deep learning? . Lots of math - False | Lots of data - False | Lots of expensive computers - False | A PhD - False | . Name five areas where deep learning is now the best in the world. . Any five of the following: . Natural Language Processing (NLP) – Question Answering, Document Summarization and Classification, etc. | Computer Vision – Satellite and drone imagery interpretation, face detection and recognition, image captioning, etc. | Medicine – Finding anomalies in medical images (ex: CT, X-ray, MRI), detecting features in tissue slides (pathology), diagnosing diabetic retinopathy, etc. | Biology – Folding proteins, classifying, genomics tasks, cell classification, etc. | Image generation/enhancement – colorizing images, improving image resolution (super-resolution), removing noise from images (denoising), converting images to art in style of famous artists (style transfer), etc. | Recommendation systems – web search, product recommendations, etc. | Playing games – Super-human performance in Chess, Go, Atari games, etc | Robotics – handling objects that are challenging to locate (e.g. transparent, shiny, lack of texture) or hard to pick up | Other applications – financial and logistical forecasting; text to speech; much much more. | . What was the name of the first device that was based on the principle of the artificial neuron? . Mark I perceptron built by Frank Rosenblatt . Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? . A set of processing units | A state of activation | An output function for each unit | A pattern of connectivity among units | A propagation rule for propagating patterns of activities through the network of connectivities | An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit | A learning rule whereby patterns of connectivity are modified by experience | An environment within which the system must operate | . What were the two theoretical misunderstandings that held back the field of neural networks? . In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, “Perceptrons”, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter. . In the 1980’s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential. . What is a GPU? . GPU stands for Graphics Processing Unit (also known as a graphics card). Standard computers have various components like CPUs, RAM, etc. CPUs, or central processing units, are the core units of all standard computers, and they execute the instructions that make up computer programs. GPUs, on the other hand, are specialized units meant for displaying graphics, especially the 3D graphics in modern computer games. The hardware optimizations used in GPUs allow it to handle thousands of tasks at the same time. Incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU. . Open a notebook and execute a cell containing: 1+1. What happens? . In a Jupyter Notebook, we can create code cells and run code in an interactive manner. When we execute a cell containing some code (in this case: 1+1), the code is run by Python and the output is displayed underneath the code cell (in this case: 2). . Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. . See Above . . . . Complete the Jupyter Notebook online appendix. . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. | Why is it hard to use a traditional computer program to recognize images in a photo? . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. | What did Samuel mean by &quot;weight assignment&quot;? . “weight assignment” refers to the current values of the model parameters. Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ” and a “ mechanism for altering the weight assignment so as to maximize the performance ”. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance. . What term do we normally use in deep learning for what Samuel called &quot;weights&quot;? . We instead use the term parameters. In deep learning, the term “weights” has a separate meaning. (The neural network has various parameters that we fit our data to. As shown in upcoming chapters, the two types of neural network parameters are weights and biases) . Draw a picture that summarizes Samuel&#39;s view of a machine learning model. . . Why is it hard to understand why a deep learning model makes a particular prediction? . This is a highly-researched topic known as interpretability of deep learning models. Deep learning models are hard to understand in part due to their “deep” nature. Think of a linear regression model. Simply, we have some input variables/data that are multiplied by some weights, giving us an output. We can understand which variables are more important and which are less important based on their weights. A similar logic might apply for a small neural network with 1-3 layers. However, deep neural networks have hundreds, if not thousands, of layers. It is hard to determine which factors are important in determining the final output. The neurons in the network interact with each other, with the outputs of some neurons feeding into other neurons. Altogether, due to the complex nature of deep learning models, it is very difficult to understand why a neural network makes a given prediction. . However, in some cases, recent research has made it easier to better understand a neural network’s prediction. For example, as shown in this chapter, we can analyze the sets of weights and determine what kind of features activate the neurons. When applying CNNs to images, we can also see which parts of the images highly activate the model. We will see how we can make our models interpretable later in the book. . What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . The universal approximation theorem states that neural networks can theoretically represent any mathematical function. However, it is important to realize that practically, due to the limits of available data and computer hardware, it is impossible to practically train a model to do so. But we can get very close! . What do you need in order to train a model? . You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer). . How could a feedback loop impact the rollout of a predictive policing model? . In a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop . Do we always have to use 224×224-pixel images with the cat recognition model? . No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption. . What is the difference between classification and regression? . Classification is focused on predicting a class or category (ex: type of pet). Regression is focused on predicting a numeric quantity (ex: age of pet). . What is a validation set? What is a test set? Why do we need them? . The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data. . What will fastai do if you don&#39;t provide a validation set? . fastai will automatically create a validation dataset. It will randomly take 20% of the data and assign it as the validation set ( valid_pct = 0.2 ). . Can we always use a random sample for a validation set? Why or why not? . A good validation or test set should be representative of new data you will see in the future. Sometimes this isn’t true if a random sample is used. For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach. . What is overfitting? Provide an example. . Overfitting is the most challenging issue when it comes to training machine learning models. Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data. This is especially important when it comes to neural networks, because neural networks can potentially “memorize” the dataset that the model was trained on, and will perform abysmally on unseen data because it didn’t “memorize” the ground truth values for that data. This is why a proper validation framework is needed by splitting the data into training, validation, and test sets. . What is a metric? How does it differ from &quot;loss&quot;? . A metric is a function that measures quality of the model’s predictions using the validation set. This is similar to the ­ loss , which is also a measure of performance of the model. However, loss is meant for the optimization algorithm (like SGD) to efficiently update the model parameters, while metrics are human-interpretable measures of performance. Sometimes, a metric may also be a good choice for the loss. . How can pretrained models help? . Pretrained models have been trained on other problems that may be quite similar to the current task. For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects. Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection. However, since the model was trained for a different task than already used, this model cannot be used as is. . What is the &quot;head&quot; of a model? . When using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model. . What kinds of features do the early layers of a CNN find? How about the later layers? . Earlier layers learn simple features like diagonal, horizontal, and vertical edges. Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals. . Are image models only useful for photos? . No! Image models can be useful for other types of images like sketches, medical data, etc. . However, a lot of information can be represented as images . For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio. Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification. There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem, and use pretrained image models to obtain state-of-the-art results! . What is an &quot;architecture&quot;? . The architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit. . What is segmentation? . At its core, segmentation is a pixelwise classification problem. We attempt to predict a label for every single pixel in the image. This provides a mask for which parts of the image correspond to the given label. . What is y_range used for? When do we need it? . y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5). . What are &quot;hyperparameters&quot;? . Training models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters. . What&#39;s the best way to avoid failures when using AI in an organization? . Key things to consider when using AI in an organization: . Make sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner. | Try out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases. | .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/02/Greetings-Programs.html",
            "relUrl": "/jupyter/2020/12/02/Greetings-Programs.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "HAL O World",
            "content": "Learning to Deep Learn . . . So today is a monumental day in the field of deep learning, AlphaFold by DeepMind has been recognised as a solution to the “protein folding problem” and marks the end of a challenge set over 50 years ago. This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world. So now is as good a time as any to really get involved and start learning to Deep Learn. . Deep Learning for Coders . To ensure that I am making good progress, creating practical models and work as effectiently as possible I will be working through the book &quot;Deep Learning for Coders with fastai and PyTorch&quot;, not only is there a great online portal with a video walk through of the curriculum, an incredible and very well presented book, but also all the source (not only for the practical work, but also for the entire book! It was all written in jupyter notebooks, how amazing is that!). . Getting Setup . So far I have made it through a couple of chapters of the book, and as reccomended by the course I am starting a &quot;Deep Learning Journal&quot;, just a place online that I can track my progress, thoughts and experiments as I progress through the curriculum. There are a couple of steps that are required to get setup to make the most of the work presented, and I thought that a good place to start a blog is with some notes on how to get setup. . Juypter Notebooks . Working as a Technical Animator in video games and having written Python code for nearly a decade now, I was aware of juypter notebooks, but I felt that as it was only useful for creating &quot;pretty comments&quot; inline with your code. Yes it would be cool to have an animated gif as part of your documentation but how useful are they in reality? It turns out that they are awesome! I am writing this very blog post using them right now and I am really impressed. The fastai team have created this entire blogging platform fastpages to help people get up and running quickly. Jeremy from fastai has also created a great talk about the power of notebooks and their platform nbdev here which is worth a watch. . Anaconda Navigator . Getting set up with juypter on Windows 10 is painless, there is an incredible tool called Anaconda Navaigator, an all in one data science toolkit for python that can also manage your packages and your environments through a simple interface. No more package dependency hell, especially when installing large packages for deep learning. Simply follow the link above and download the graphical installer for your OS. Once installed, you can launch your juypter notebook environment here: . . Fastpages . Fastpages the platform I am using to setup your blog is incredibly simple and the team at fast.ai have documented the process amazingly, simply stepping through the documentation will allow you to copy their directory as a template along with all of the GitHub actions that will build and display your blog. You can see the documentation here . . Paperspace Gradient . Another fantastic facet of the course is that they don&#39;t encourage you to wade into the murky world of systems administration setting up packages, environments and GPU compliant kernels to get you up and running working with state-of-the-art deep learning models. They instead encourage you to look into using a Server Notebook, they reccommend either Collab, Gradient or Sagemaker. I chose Paperspace Gradient and I am very happy with my choice. . Microsoft Azure . By far the most frustrating part of the set up for the course has nothing to do with Python, the notebook server, GitHub or fast.ai. In Chapter 2 of the book, the course encourages you to set up a Microsoft Azure account to get an API key so you can use their Image Search conginitive service to pull training images from the web to train a classifier model. Unfortunately the documentation on the site and the forum isn&#39;t incredibly helpful, so I am logging it here incase anyone should find it useful, but mainly if I have to do this again on another account I can remeber how and where to find everything. . Go to: https://azure.microsoft.com/en-gb/services/cognitive-services/ . . Scroll to the API&#39;s section of the page, and select &quot;Web Search&quot; . . This will redirect you to https://www.microsoft.com/en-us/bing/apis/bing-web-search-api . . Ensuring you already have an Azure account, this will take you to the console to create a Bing resource . . Once you have filled in the details and created your resource it will take you to your Azure dashboard . . Go to the &quot;Keys and Endpoint&quot; tab, this is where you can find your Bing Image Search Api Key . . Now that you have your API key and replaced the XXX value in the Chapter 2 notebook, there is once more piece of code required to get the code in Chapter to work, below is a re-write of the search_images_bing definition, simply paste it as a code cell above where it is being called in notebook. Everything should work for you now. . def search_images_bing(subscription_key, search_term, size = 150): search_url = &quot;https://api.bing.microsoft.com/v7.0/images/search&quot; headers = {&quot;Ocp-Apim-Subscription-Key&quot; : subscription_key} params = {&quot;q&quot;: search_term, &quot;license&quot;: &quot;public&quot;, &quot;imageType&quot;: &quot;photo&quot;, &quot;count&quot;: size} response = requests.get(search_url, headers=headers, params=params) response.raise_for_status() search_results = response.json() reformatted_results = L(search_results[&quot;value&quot;], use_list=True) # Uses the FastAI class L, a # &quot;drop-in&quot; replacement for Lists. Mixes Python standard library and numpy arrays. # I&#39;m putting this here so, again, we minimize the amount of individual cells rewritten. # Many of the later cells assume .attrgot is a valid thing you can call. for result in reformatted_results: result[&quot;content_url&quot;] = result[&quot;contentUrl&quot;] # Bing changed their API. They return contentUrl instead of content_url. # Again, this will help in the long run. return reformatted_results .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/12/01/HAL-O-World.html",
            "relUrl": "/jupyter/2020/12/01/HAL-O-World.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bailey-deep-learning.github.io/im_sorry_dave/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bailey-deep-learning.github.io/im_sorry_dave/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bailey-deep-learning.github.io/im_sorry_dave/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}